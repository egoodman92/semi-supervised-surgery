{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tools/lib/python3.7/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `SimplexNoiseAlpha()` is deprecated. Use `BlendAlphaSimplexNoise` instead. SimplexNoiseAlpha is deprecated. Use BlendAlphaSimplexNoise instead. The order of parameters is the same. Parameter 'first' was renamed to 'foreground'. Parameter 'second' was renamed to 'background'.\n",
      "  warn_deprecated(msg, stacklevel=3)\n",
      "/home/ubuntu/anaconda3/envs/tools/lib/python3.7/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `FrequencyNoiseAlpha()` is deprecated. Use `BlendAlphaFrequencyNoise` instead. FrequencyNoiseAlpha is deprecated. Use BlendAlphaFrequencyNoise instead. The order of parameters is the same. Parameter 'first' was renamed to 'foreground'. Parameter 'second' was renamed to 'background'.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "/home/ubuntu/surgery-tool-detection/models/\n",
      "/home/ubuntu/surgery-tool-detection/logs/\n",
      "Using new Balanced Sampler\n"
     ]
    }
   ],
   "source": [
    "###  RUN FROM SCRATCH  ### \n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils as vision_utils \n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from retinanet import model\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "    Normalizer, BalancedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from retinanet.sampler import SamplerFactory, aggregate_indices_per_class \n",
    "\n",
    "\n",
    "from retinanet import coco_eval\n",
    "from retinanet import csv_eval\n",
    "\n",
    "assert torch.__version__.split('.')[0] == '1'\n",
    "\n",
    "\n",
    "MODELS_DIR = '/home/ubuntu/surgery-tool-detection/models/'\n",
    "LOGS_DIR = '/home/ubuntu/surgery-tool-detection/logs/'\n",
    "\n",
    "##MODELS_DIR = str(Path(__file__).resolve().parents[2]) + '/models/'\n",
    "##LOGS_DIR = str(Path(__file__).resolve().parents[2]) + '/logs/'\n",
    "# pretrained_model = MODELS_DIR + 'coco_resnet_50_map_0_335_state_dict.pt'\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(MODELS_DIR) \n",
    "\n",
    "print(LOGS_DIR) \n",
    "\n",
    "\n",
    "### Set up model configs ### \n",
    "\n",
    "parser = argparse.ArgumentParser(description='Simple training script for training a RetinaNet network.')\n",
    "\n",
    "parser.add_argument('--dataset', help='Dataset type, must be one of csv or coco.')\n",
    "parser.add_argument('--coco_path', help='Path to COCO directory')\n",
    "parser.add_argument('--csv_train', help='Path to file containing training annotations (see readme)')\n",
    "parser.add_argument('--csv_classes', help='Path to file containing class list (see readme)')\n",
    "parser.add_argument('--csv_val', help='Path to file containing validation annotations (optional, see readme)')\n",
    "\n",
    "parser.add_argument('--depth', help='Resnet depth, must be one of 18, 34, 50, 101, 152', type=int, default=50)\n",
    "parser.add_argument('--epochs', help='Number of epochs', type=int, default=100)\n",
    "parser.add_argument('--model_name', help='Name to store the trianed model under.')\n",
    "parser.add_argument('--log_output', help='Save output to csv file', action='store_true')\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--learning_rate', type=float)\n",
    "parser.add_argument('--threshold', help='iou threshold to count as detection')\n",
    "parser.add_argument('--sampler', help='Type of sampler to use, default aspect ratio sampler.')\n",
    "parser.add_argument('--augment', action='store_true')\n",
    "parser.add_argument('--pretrained', help='Path to pretrained model')\n",
    "parser.add_argument('--blacken', action='store_true')\n",
    "\n",
    "#parser = parser.parse_args(args)\n",
    "\n",
    "\n",
    "##### PRIMARY MODEL CONFIGS TO SET ### \n",
    "\n",
    "parser.model_name = 'Augie' \n",
    "parser.depth = 50 \n",
    "parser.epochs = 1 \n",
    "\n",
    "#parser.sampler = None \n",
    "#parser.sampler = \"old balanced (no alpha)\" \n",
    "parser.sampler = 'new balanced' \n",
    "\n",
    "parser.alpha = 0.0 \n",
    "parser.augment = 0.0 #False \n",
    "parser.pipeline = \"lighting\" #False \n",
    "\n",
    "##### END PRIMARY CONFIGS ######## \n",
    "\n",
    "parser.log_output = True \n",
    "LOG_DIR = LOGS_DIR + parser.model_name + '/'\n",
    "os.makedirs(LOG_DIR, exist_ok=True) \n",
    "\n",
    "parser.dataset = 'csv' \n",
    "parser.csv_train = '/home/ubuntu/surgery-tool-detection/src/data/train_data.csv' #'../data/train_data.csv'\n",
    "parser.csv_val = '/home/ubuntu/surgery-tool-detection/src/data/val_data.csv' #../val_data.csv \n",
    "parser.csv_raw = '/home/ubuntu/surgery-tool-detection/src/data/raw_data.csv' ### \n",
    "parser.csv_classes = '/home/ubuntu/surgery-tool-detection/src/data/class_names_prune.csv' #../data/class_names.csv \n",
    "praser.csv_test = '/home/ubuntu/surgery-tool-detection/src/data/test_data.csv'\n",
    "\n",
    "\n",
    "parser.blacken = False \n",
    "parser.learning_rate = None \n",
    "\n",
    "parser.batch_size = None ## Set and log config? \n",
    "parser.threshold = None ## Set and log config? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'model'\n",
    "if parser.model_name is not None:\n",
    "    model_name = parser.model_name\n",
    "\n",
    "learning_rate = 1e-5\n",
    "if parser.learning_rate is not None:\n",
    "    learning_rate = float(parser.learning_rate)\n",
    "\n",
    "batch_size = 2\n",
    "if parser.batch_size is not None:\n",
    "    batch_size = int(parser.batch_size)\n",
    "\n",
    "threshold = 0.5\n",
    "if parser.threshold is not None:\n",
    "    threshold = float(parser.threshold)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create the dataloaders     \n",
    "\n",
    "if parser.dataset == 'coco':\n",
    "\n",
    "    if parser.coco_path is None:\n",
    "        raise ValueError('Must provide --coco_path when training on COCO,')\n",
    "\n",
    "    dataset_train = CocoDataset(parser.coco_path, set_name='train2017',\n",
    "                                transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "    dataset_val = CocoDataset(parser.coco_path, set_name='val2017',\n",
    "                              transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "elif parser.dataset == 'csv':\n",
    "    \n",
    "    dataset_raw = CSVDataset(train_file=parser.csv_raw, class_list=parser.csv_classes, \n",
    "                             transform=transforms.Compose([Augmenter(), Resizer()]), \n",
    "                               augment=parser.augment, pipeline=parser.pipeline, blacken=parser.blacken) \n",
    "    \n",
    "\n",
    "    if parser.csv_train is None:\n",
    "        raise ValueError('Must provide --csv_train when training on COCO,')\n",
    "\n",
    "    if parser.csv_classes is None:\n",
    "        raise ValueError('Must provide --csv_classes when training on COCO,')\n",
    "\n",
    "#     dataset_train = CSVDataset(train_file=parser.csv_train, class_list=parser.csv_classes,\n",
    "#                                transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]), \n",
    "#                                augment=parser.augment, pipeline=parser.pipeline, blacken=parser.blacken)\n",
    "    dataset_train = CSVDataset(train_file=parser.csv_train, class_list=parser.csv_classes,\n",
    "                               transform=transforms.Compose([Augmenter(), Resizer()]), \n",
    "                               augment=parser.augment, pipeline=parser.pipeline, blacken=parser.blacken)\n",
    "    train_acc_set = CSVDataset(train_file=parser.csv_train, class_list=parser.csv_classes,\n",
    "                               transform=transforms.Compose([Normalizer(), Resizer()]), blacken=parser.blacken)\n",
    "    \n",
    "    dataset_test = CSVDataset(train_file=parser.csv_test, class_list=parser.csv_classes,\n",
    "                                 transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "    \n",
    "\n",
    "    if parser.csv_val is None:\n",
    "        dataset_val = None\n",
    "        print('No validation annotations provided.')\n",
    "    else:\n",
    "        dataset_val = CSVDataset(train_file=parser.csv_val, class_list=parser.csv_classes,\n",
    "                                 transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "else:\n",
    "    raise ValueError('Dataset type not understood (must be csv or coco), exiting.')\n",
    "    \n",
    "\n",
    "\n",
    "sampler = AspectRatioBasedSampler(dataset_train, batch_size=batch_size, drop_last=False)\n",
    "if parser.sampler is not None and parser.sampler == 'old balanced (no alpha)':\n",
    "    sampler = BalancedSampler(dataset_train, batch_size=batch_size, drop_last=False) \n",
    "if parser.sampler is not None and parser.sampler == 'new balanced': \n",
    "    class_indices_dict = aggregate_indices_per_class(dataset_train) \n",
    "    class_indices = list(class_indices_dict.values()) \n",
    "    # print(len(class_indices)) \n",
    "    # for label in class_indices: print(len(label))\n",
    "    #batch_sz = 2 \n",
    "    # print(\"Hello\") \n",
    "    # print(len(dataset_train)//batch_sz) \n",
    "    sampler = SamplerFactory().get(\n",
    "        class_idxs=class_indices,\n",
    "        batch_size=batch_size,\n",
    "        n_batches=len(dataset_train)//batch_size,\n",
    "        alpha=parser.alpha,\n",
    "        kind='random'\n",
    "    ) \n",
    "    \n",
    "    \n",
    "dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler) \n",
    "\n",
    "\n",
    "if dataset_val is not None:\n",
    "    sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "    dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retinanet = \n",
    "\n",
    "mAP, pr_curve = csv_eval.evaluate(dataset_test, retinanet, iou_threshold=threshold)\n",
    "\n",
    "print(mAP) \n",
    "print(pr_curve) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tools",
   "language": "python",
   "name": "tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
