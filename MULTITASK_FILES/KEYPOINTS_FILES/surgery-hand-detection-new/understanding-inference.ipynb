{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunrise-leather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script!\n",
      "/home/egoodman/multitaskmodel/scripts\n",
      "We are using torch version 1.8.0\n",
      "We are using torchvision version 0.9.0\n",
      "Studying ['slap']\n",
      "Studying video at path ./output/slap.mp4\n",
      "Creating action head\n",
      "Grabbing dimensions\n",
      "Original dimensions are [tensor([0]), tensor([0]), tensor([0])]\n",
      "\n",
      "\n",
      "    Video has following parameters\n",
      "    Frame Height : 384\n",
      "    Frame Width : 384\n",
      "    FPS : 30.0\n",
      "    Number of Frames : 539\n",
      "    Duration (seconds) : 17.966666666666665\n",
      "\n",
      "    \n",
      "Starting inference on video slap.mp4 at time 58497.554966035\n",
      "Original dimensions are [tensor([0]), tensor([0]), tensor([0])]\n",
      "Studying batch 0 torch.Size([64, 3, 384, 384])\n",
      "Drawing rectangle 78.85747528076172 71.44671630859375 137.85812377929688 127.92196655273438\n",
      "Drawing rectangle 70.24629211425781 85.19050598144531 131.17945861816406 138.34402465820312\n",
      "Original dimensions are [tensor([0]), tensor([0]), tensor([0])]\n",
      "Studying batch 1 torch.Size([64, 3, 384, 384])\n",
      "Original dimensions are [tensor([0]), tensor([0]), tensor([0])]\n",
      "Studying batch 2 torch.Size([64, 3, 384, 384])\n",
      "Drawing rectangle 70.13934326171875 39.63982391357422 167.44395446777344 132.54986572265625\n",
      "Drawing rectangle 76.81687927246094 44.87136459350586 236.6613311767578 125.98896789550781\n",
      "Drawing rectangle 75.2927474975586 42.93228530883789 224.87094116210938 125.86027526855469\n",
      "Drawing rectangle 76.96345520019531 49.02204513549805 218.42466735839844 122.86614990234375\n",
      "Drawing rectangle 80.95326232910156 45.81697463989258 202.8668670654297 108.53450012207031\n",
      "Drawing rectangle 82.32122802734375 44.91103744506836 203.05911254882812 110.1395263671875\n",
      "Drawing rectangle 83.19755554199219 42.041473388671875 201.71559143066406 111.82150268554688\n",
      "Drawing rectangle 83.59864807128906 43.39084243774414 203.61732482910156 111.94975280761719\n",
      "\n",
      " ...inference complete after 3.795158929002355 !\n",
      "Output video is  ./output/slap_detections.mp4\n",
      "Returning inference outputs (17.966666666666665, 3.795158929002355, [tensor([0]), tensor([0]), tensor([0])])\n",
      "Video was 17.966666666666665 seconds, and inference was performed in 3.795158929002355 seconds\n",
      "Printing output ['slap', 17.966666666666665, 3.795158929002355, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#example use: python inference.py --vid_name=hyw7Ue6oW8w.mp4\n",
    "\n",
    "print(\"Starting script!\")\n",
    "\n",
    "%cd /home/egoodman/multitaskmodel/scripts\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_CATEGORIES = ['cutting', 'tying', 'suturing', 'background']\n",
    "DETECTION_CLASSES = ['bovie', 'forceps', 'needledriver', 'hand']\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/egoodman/multitaskmodel/MULTITASK_FILES/TSM_FILES/\")\n",
    "sys.path.append('/home/egoodman/multitaskmodel/MULTITASK_FILES/RETINANET_FILES/src/pytorch-retinanet/')\n",
    "\n",
    "from dataset import * #imports dataloaders from TSM\n",
    "SurgeryDataset.categories = DEFAULT_CATEGORIES\n",
    "from train import get_train_val_data_loaders, run_epoch\n",
    "from model import get_model_name, save_model, save_results, get_model\n",
    "from barbar import Bar\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "import utils\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import timeit\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Surgery Hand and Keypoint Detection on Video')\n",
    "    parser.add_argument('--vid_name', type=str, default=\"slap.mp4\")\n",
    "    parser.add_argument('--directory', type=str, default=\"./output/\")\n",
    "    parser.add_argument('--tool_model_loc', type=str, default=\"/home/egoodman/multitaskmodel/logs/best_models/20210427_bigmultitask_basicactionhead_352h64d_newinference_99_incomplete.pt\")\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_test_data_loaders(segments_df, batch_size, data_dir='data/', model='TSM', pre_crop_size=352, segment_length=5,\n",
    "                                                                    aug_method='val'):\n",
    "    df = segments_df.sort_values(by=['video_id', 'start_seconds'])\n",
    "    test_dataset = SurgeryDataset(df, data_dir=data_dir, mode='test', model=model, balance=False,\n",
    "                                   pre_crop_size=pre_crop_size, aug_method=aug_method)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=False)\n",
    "    return test_data_loader\n",
    "\n",
    "\n",
    "\n",
    "def get_video_path(video_id, data_dir='data/'):\n",
    "    return os.path.join(data_dir + video_id + \".mp4\")\n",
    "\n",
    "\n",
    "\n",
    "def get_video_duration(filename):\n",
    "    video = cv2.VideoCapture(filename)\n",
    "    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    return frame_count, fps\n",
    "\n",
    "\n",
    "\n",
    "def anns_from_video_ids(video_ids, data_dir, segment_length, offset=0):\n",
    "    print(\"Studying\", video_ids)\n",
    "    rows = []\n",
    "    for video_id in video_ids:\n",
    "        video_path = get_video_path(video_id, data_dir)\n",
    "        print(\"Studying video at path\", video_path)\n",
    "        if not os.path.exists(video_path):\n",
    "            print(\"Video not downloaded: %s\" % video_id)\n",
    "            continue\n",
    "        frame_count, fps = get_video_duration(video_path)\n",
    "        num_anns = int(frame_count / fps / segment_length)\n",
    "        for i in range(num_anns):\n",
    "            start_seconds = offset + i * segment_length\n",
    "            label = 'background'\n",
    "            row = {'start_seconds': start_seconds,\n",
    "                   'video_id': video_id,\n",
    "                   'end_seconds': start_seconds + segment_length,\n",
    "                   'duration': segment_length,\n",
    "                   'label': label,\n",
    "                   'category': label}\n",
    "            rows.append(row)\n",
    "    anns_df = pd.DataFrame(rows)\n",
    "    return anns_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from retinanet import model_3_heads\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "print(\"We are using torch version\", torch.__version__)\n",
    "print(\"We are using torchvision version\", torchvision.__version__)\n",
    "\n",
    "\n",
    "#color scheme: hands(gold), bovie(red), needledriver(blue), forceps(green)\n",
    "annot_to_color = {0 : (255, 0, 0), 1 : (0, 255, 0), 2 : (0, 0, 255), 3 : (255, 215, 0) }\n",
    "\n",
    "\n",
    "def inference(test_data_loader, vid_name, tool_model_loc, directory):\n",
    "\n",
    "    #used for measuring the inference time\n",
    "    device = torch.device(\"cuda\")\n",
    "    retinanet = model_3_heads.resnet50(num_classes=4)\n",
    "    retinanet = torch.load(tool_model_loc) \n",
    "    retinanet.to(device)\n",
    "\n",
    "    tool_model = retinanet\n",
    "    if torch.cuda.is_available():\n",
    "        tool_model = tool_model.cuda()\n",
    "        tool_model = torch.nn.DataParallel(tool_model).cuda()\n",
    "        tool_model.eval()\n",
    "\n",
    "    #this is just to get the dimensions of the dataset we're working with! only done at very beginning\n",
    "    for iter_num, (data_action, record_ids, action_labels) in enumerate(test_data_loader):\n",
    "        print(\"Grabbing dimensions\")\n",
    "        original_dimensions = record_ids[1]\n",
    "        print(\"Original dimensions are\", record_ids[1])\n",
    "        break\n",
    "\n",
    "    data_action = (data_action.view((-1, 3) + data_action.size()[-2:]))\n",
    "    b, c, height, width = data_action.shape\n",
    "    \n",
    "    #video for superposition of detections/actions\n",
    "    video = directory + vid_name\n",
    "    video = cv2.VideoCapture(video)\n",
    "    fps_video = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    #params for output video and json\n",
    "    out_video = directory + vid_name[:-4] + \"_detections.mp4\"\n",
    "    fps = 13 #frame rate defined by model\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_tracked = cv2.VideoWriter(out_video, fourcc, fps, (int(width), int(height)))\n",
    "    output_json = defaultdict(list)\n",
    "\n",
    "    #params for clean out video for tracking\n",
    "    clean_out_video = directory + vid_name[:-4] + \"_nodetections.mp4\"\n",
    "    fps = 13\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_clean = cv2.VideoWriter(clean_out_video, fourcc, fps, (int(width), int(height)))\n",
    "\n",
    "\n",
    "    print('''\n",
    "\n",
    "    Video has following parameters\n",
    "    Frame Height : {}\n",
    "    Frame Width : {}\n",
    "    FPS : {}\n",
    "    Number of Frames : {}\n",
    "    Duration (seconds) : {}\n",
    "\n",
    "    '''.format(height, width, fps_video, frame_count, frame_count/fps_video))\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    print(\"Starting inference on video {} at time {}\".format(vid_name, start_time))\n",
    "\n",
    "    for iter_num, (data_action, record_ids, action_labels) in enumerate(test_data_loader):\n",
    "        print(\"Original dimensions are\", record_ids[1])\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #reshape data and forward pass!\n",
    "            data_action = (data_action.view((-1, 3) + data_action.size()[-2:]))\n",
    "            print(\"Studying batch\", iter_num, data_action.shape)\n",
    "            batch_nms_scores, batch_nms_class, batch_transformed_anchors, action_logits = tool_model(data_action)\n",
    "            \n",
    "            #go frame by frame through output and add to video\n",
    "            for frame_no in range(len(batch_nms_scores)):\n",
    "                frame_detections = []\n",
    "\n",
    "                nms_scores = batch_nms_scores[frame_no]\n",
    "                transformed_anchors = batch_transformed_anchors[frame_no]\n",
    "                nms_class = batch_nms_class[frame_no]\n",
    "                                \n",
    "                idxs = np.where(nms_scores.cpu() >= .5)\n",
    "\n",
    "                frame = data_action.squeeze().cuda().float()[frame_no, :, :, :].cpu().numpy()\n",
    "                frame = np.transpose(frame, (1,2,0))\n",
    "                frame = np.array(255*(frame.copy() *np.array([[[0.2650, 0.2877, 0.3311]]]) + np.array([[[0.3051, 0.3570, 0.4115]]])), dtype = np.uint8  )\n",
    "\n",
    "                video_clean.write(np.uint8(frame))\n",
    "\n",
    "\n",
    "                if len(idxs[0]) > 0:\n",
    "                    for idx in idxs[0]:\n",
    "                        a = float(transformed_anchors[idx].detach()[0])\n",
    "                        b = float(transformed_anchors[idx].detach()[1])\n",
    "                        c = float(transformed_anchors[idx].detach()[2])\n",
    "                        d = float(transformed_anchors[idx].detach()[3])\n",
    "                        e = float(nms_class[idx].detach()) #this last coordinate is the ID\n",
    "                        \n",
    "                        print(\"Drawing rectangle\", a, b, c, d)\n",
    "\n",
    "                        #draws the actual rectangle\n",
    "                        cv2.rectangle(frame, (int(a), int(b)), (int(c), int(d)), color=annot_to_color[int(e)], thickness=3)\n",
    "                        object_dict = {DETECTION_CLASSES[int(e)] : [a, b, c, d, float(nms_scores[idx])]}\n",
    "                        frame_detections.append(object_dict)\n",
    "\n",
    "                #used for labeling action\n",
    "                cur_action = DEFAULT_CATEGORIES[int(torch.argmax(action_logits))]\n",
    "                cur_action_prob = torch.max(action_logits)\n",
    "\n",
    "                actions_vis = True\n",
    "                if actions_vis:\n",
    "                    cv2.putText(frame, cur_action + \" \" + str(float(cur_action_prob.data))[:5], (50, 50), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color = (255, 255, 255) )\n",
    "\n",
    "                #add all these annotations to the video or json\n",
    "                video_tracked.write(np.uint8(frame))\n",
    "\n",
    "                output_json[int(64*iter_num + frame_no)] = {\"actions\" : (cur_action, str(float(cur_action_prob.data))[:5]), \\\n",
    "                                                          \"detections\" : frame_detections}\n",
    "    \n",
    "    #calculate how fast our inference was!\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print('\\n ...inference complete after {} !'.format(elapsed))\n",
    "    \n",
    "    #release the video and the json!\n",
    "    video_tracked.release()\n",
    "    video_clean.release()\n",
    "\n",
    "    with open(directory + vid_name[:-4] + \"_detections.json\", \"w\") as outfile:\n",
    "        json.dump(output_json, outfile)\n",
    "    print(\"Output video is \", out_video)\n",
    "    \n",
    "    return frame_count/fps_video, elapsed, original_dimensions\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    segments_df = anns_from_video_ids([args.vid_name[:-4]], args.directory, segment_length=5)\n",
    "    test_data_loader = get_test_data_loaders(segments_df, batch_size=1, data_dir = args.directory)\n",
    "    inference_outputs = inference(test_data_loader, args.vid_name, args.tool_model_loc, args.directory)\n",
    "\n",
    "    print(\"Returning inference outputs\", inference_outputs)\n",
    "\n",
    "    print(\"Video was {} seconds, and inference was performed in {} seconds\".format(inference_outputs[0], inference_outputs[1]))\n",
    "    output = [args.vid_name[:-4], inference_outputs[0], inference_outputs[1], \\\n",
    "             float(inference_outputs[2][0]), float(inference_outputs[2][1]), float(inference_outputs[2][2])]\n",
    "    print(\"Printing output\", output)\n",
    "\n",
    "    with open(args.vid_name[:-4]+'_times.txt', 'w') as filehandle:\n",
    "        for listitem in output:\n",
    "            filehandle.write('%s ' % listitem)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multitaskenv",
   "language": "python",
   "name": "multitaskenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
