{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedicated-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using torch version 1.8.0\n",
      "We are using torchvision version 0.9.0\n",
      "/home/egoodman/keypoints/surgery-hand-detection\n",
      "/home/egoodman/keypoints/surgery-hand-detection\n",
      "Getting pose model from pose_hrnet.get_pose_net\n",
      "Using cfg AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: coco\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: False\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 10\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/coco/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: val\n",
      "  TRAIN_SET: train_boot_aug_2\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: True\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  EXTRA:\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "    STAGE2:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4]\n",
      "      NUM_BRANCHES: 2\n",
      "      NUM_CHANNELS: [48, 96]\n",
      "      NUM_MODULES: 1\n",
      "    STAGE3:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4]\n",
      "      NUM_BRANCHES: 3\n",
      "      NUM_CHANNELS: [48, 96, 192]\n",
      "      NUM_MODULES: 4\n",
      "    STAGE4:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4, 4]\n",
      "      NUM_BRANCHES: 4\n",
      "      NUM_CHANNELS: [48, 96, 192, 384]\n",
      "      NUM_MODULES: 3\n",
      "  HEATMAP_SIZE: [72, 96]\n",
      "  IMAGE_SIZE: [288, 384]\n",
      "  INIT_WEIGHTS: True\n",
      "  NAME: pose_hrnet\n",
      "  NUM_JOINTS: 21\n",
      "  PRETRAINED: deep-high-resolution-net.pytorhc/models/trained/cmu_48_best.pth\n",
      "  SIGMA: 3\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 1000\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 12\n",
      "  BBOX_THRE: 1.0\n",
      "  COCO_BBOX_FILE: data/coco/hand_detection_results/coco_instances_results_72.json\n",
      "  FLIP_TEST: False\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: keypoints_final.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SHIFT_HEATMAP: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 12\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 300\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_FACTOR: 0.1\n",
      "  LR_STEP: [170, 200]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 24\n",
      "HERE, CFG IS AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: coco\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: False\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 10\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/coco/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: val\n",
      "  TRAIN_SET: train_boot_aug_2\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: True\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  EXTRA:\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "    STAGE2:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4]\n",
      "      NUM_BRANCHES: 2\n",
      "      NUM_CHANNELS: [48, 96]\n",
      "      NUM_MODULES: 1\n",
      "    STAGE3:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4]\n",
      "      NUM_BRANCHES: 3\n",
      "      NUM_CHANNELS: [48, 96, 192]\n",
      "      NUM_MODULES: 4\n",
      "    STAGE4:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4, 4]\n",
      "      NUM_BRANCHES: 4\n",
      "      NUM_CHANNELS: [48, 96, 192, 384]\n",
      "      NUM_MODULES: 3\n",
      "  HEATMAP_SIZE: [72, 96]\n",
      "  IMAGE_SIZE: [288, 384]\n",
      "  INIT_WEIGHTS: True\n",
      "  NAME: pose_hrnet\n",
      "  NUM_JOINTS: 21\n",
      "  PRETRAINED: deep-high-resolution-net.pytorhc/models/trained/cmu_48_best.pth\n",
      "  SIGMA: 3\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 1000\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 12\n",
      "  BBOX_THRE: 1.0\n",
      "  COCO_BBOX_FILE: data/coco/hand_detection_results/coco_instances_results_72.json\n",
      "  FLIP_TEST: False\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: keypoints_final.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SHIFT_HEATMAP: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 12\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 300\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_FACTOR: 0.1\n",
      "  LR_STEP: [170, 200]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 24\n",
      "extra is FINAL_CONV_KERNEL: 1\n",
      "PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "STAGE2:\n",
      "  BLOCK: BASIC\n",
      "  FUSE_METHOD: SUM\n",
      "  NUM_BLOCKS: [4, 4]\n",
      "  NUM_BRANCHES: 2\n",
      "  NUM_CHANNELS: [48, 96]\n",
      "  NUM_MODULES: 1\n",
      "STAGE3:\n",
      "  BLOCK: BASIC\n",
      "  FUSE_METHOD: SUM\n",
      "  NUM_BLOCKS: [4, 4, 4]\n",
      "  NUM_BRANCHES: 3\n",
      "  NUM_CHANNELS: [48, 96, 192]\n",
      "  NUM_MODULES: 4\n",
      "STAGE4:\n",
      "  BLOCK: BASIC\n",
      "  FUSE_METHOD: SUM\n",
      "  NUM_BLOCKS: [4, 4, 4, 4]\n",
      "  NUM_BRANCHES: 4\n",
      "  NUM_CHANNELS: [48, 96, 192, 384]\n",
      "  NUM_MODULES: 3\n",
      "=> loading model from keypoints_final.pth\n",
      "Opening slap.mp4\n",
      "We are going to produce the video True\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[331.9477, 267.8124, 359.6288, 321.9651],\n",
      "        [ 51.9240,  69.3943, 140.5634, 142.5141],\n",
      "        [127.8748,  67.4037, 240.6877, 202.7232],\n",
      "        [ 88.7073, 283.7643, 298.6220, 355.7100]], device='cuda:0')), scores: tensor([0.9811, 0.9505, 0.9190, 0.7993], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[331.9477, 267.8124, 359.6288, 321.9651],\n",
      "        [ 51.9240,  69.3943, 140.5634, 142.5141],\n",
      "        [127.8748,  67.4037, 240.6877, 202.7232],\n",
      "        [ 88.7073, 283.7643, 298.6220, 355.7100]], device='cuda:0'))\n",
      "Analyzing box tensor([331.9477, 267.8124, 359.6288, 321.9651], device='cuda:0') with config parms 288 384\n",
      "Centers are [345.78827 294.88873] and scales are [0.25384054 0.33845407]\n",
      "Analyzing box tensor([ 51.9240,  69.3943, 140.5634, 142.5141], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 96.243675 105.95421 ] and scales are [0.55399656 0.7386621 ]\n",
      "Analyzing box tensor([127.8748,  67.4037, 240.6877, 202.7232], device='cuda:0') with config parms 288 384\n",
      "Centers are [184.28122 135.06345] and scales are [0.7050805  0.94010735]\n",
      "Analyzing box tensor([ 88.7073, 283.7643, 298.6220, 355.7100], device='cuda:0') with config parms 288 384\n",
      "Centers are [193.66464 319.73715] and scales are [1.3119671 1.7492895]\n",
      "SENDING IN TO POSES [array([345.78827, 294.88873], dtype=float32), array([ 96.243675, 105.95421 ], dtype=float32), array([184.28122, 135.06345], dtype=float32), array([193.66464, 319.73715], dtype=float32)] [array([0.25384054, 0.33845407], dtype=float32), array([0.55399656, 0.7386621 ], dtype=float32), array([0.7050805 , 0.94010735], dtype=float32), array([1.3119671, 1.7492895], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[331.2381, 266.9051, 359.0597, 323.7545],\n",
      "        [ 54.0837,  72.5830, 141.2646, 141.8082],\n",
      "        [129.4013,  71.2372, 239.6617, 201.5531],\n",
      "        [ 88.2002, 282.2562, 296.4032, 356.0156]], device='cuda:0')), scores: tensor([0.9791, 0.9703, 0.8446, 0.7606], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[331.2381, 266.9051, 359.0597, 323.7545],\n",
      "        [ 54.0837,  72.5830, 141.2646, 141.8082],\n",
      "        [129.4013,  71.2372, 239.6617, 201.5531],\n",
      "        [ 88.2002, 282.2562, 296.4032, 356.0156]], device='cuda:0'))\n",
      "Analyzing box tensor([331.2381, 266.9051, 359.0597, 323.7545], device='cuda:0') with config parms 288 384\n",
      "Centers are [345.14893 295.3298 ] and scales are [0.26648167 0.35530892]\n",
      "Analyzing box tensor([ 54.0837,  72.5830, 141.2646, 141.8082], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 97.67412 107.19557] and scales are [0.5448807  0.72650766]\n",
      "Analyzing box tensor([129.4013,  71.2372, 239.6617, 201.5531], device='cuda:0') with config parms 288 384\n",
      "Centers are [184.53148 136.39514] and scales are [0.6891275 0.9188367]\n",
      "Analyzing box tensor([ 88.2002, 282.2562, 296.4032, 356.0156], device='cuda:0') with config parms 288 384\n",
      "Centers are [192.3017  319.13586] and scales are [1.301269  1.7350255]\n",
      "SENDING IN TO POSES [array([345.14893, 295.3298 ], dtype=float32), array([ 97.67412, 107.19557], dtype=float32), array([184.53148, 136.39514], dtype=float32), array([192.3017 , 319.13586], dtype=float32)] [array([0.26648167, 0.35530892], dtype=float32), array([0.5448807 , 0.72650766], dtype=float32), array([0.6891275, 0.9188367], dtype=float32), array([1.301269 , 1.7350255], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[330.4837, 266.9525, 358.9575, 325.1373],\n",
      "        [ 52.2834,  73.6707, 144.5942, 141.8535],\n",
      "        [130.1968,  73.7037, 239.6924, 201.7101],\n",
      "        [ 91.4286, 281.4868, 298.2621, 355.5309]], device='cuda:0')), scores: tensor([0.9792, 0.9663, 0.9009, 0.6132], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[330.4837, 266.9525, 358.9575, 325.1373],\n",
      "        [ 52.2834,  73.6707, 144.5942, 141.8535],\n",
      "        [130.1968,  73.7037, 239.6924, 201.7101],\n",
      "        [ 91.4286, 281.4868, 298.2621, 355.5309]], device='cuda:0'))\n",
      "Analyzing box tensor([330.4837, 266.9525, 358.9575, 325.1373], device='cuda:0') with config parms 288 384\n",
      "Centers are [344.7206 296.0449] and scales are [0.2727416  0.36365545]\n",
      "Analyzing box tensor([ 52.2834,  73.6707, 144.5942, 141.8535], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.438774 107.762085] and scales are [0.5769427  0.76925695]\n",
      "Analyzing box tensor([130.1968,  73.7037, 239.6924, 201.7101], device='cuda:0') with config parms 288 384\n",
      "Centers are [184.94458 137.70691] and scales are [0.6843476  0.91246355]\n",
      "Analyzing box tensor([ 91.4286, 281.4868, 298.2621, 355.5309], device='cuda:0') with config parms 288 384\n",
      "Centers are [194.8453  318.50888] and scales are [1.2927092 1.7236124]\n",
      "SENDING IN TO POSES [array([344.7206, 296.0449], dtype=float32), array([ 98.438774, 107.762085], dtype=float32), array([184.94458, 137.70691], dtype=float32), array([194.8453 , 318.50888], dtype=float32)] [array([0.2727416 , 0.36365545], dtype=float32), array([0.5769427 , 0.76925695], dtype=float32), array([0.6843476 , 0.91246355], dtype=float32), array([1.2927092, 1.7236124], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[327.1812, 268.8648, 359.7600, 328.7418],\n",
      "        [134.2380,  74.4081, 240.3535, 204.7526],\n",
      "        [ 53.5631,  77.9800, 144.3030, 140.4466],\n",
      "        [ 93.5802, 286.6976, 300.0366, 356.2016],\n",
      "        [145.8066,   0.8140, 342.0980,  45.0438],\n",
      "        [  0.0000,   4.4593, 117.9844, 126.6399]], device='cuda:0')), scores: tensor([0.9607, 0.8410, 0.8151, 0.7570, 0.6260, 0.5453], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[327.1812, 268.8648, 359.7600, 328.7418],\n",
      "        [134.2380,  74.4081, 240.3535, 204.7526],\n",
      "        [ 53.5631,  77.9800, 144.3030, 140.4466],\n",
      "        [ 93.5802, 286.6976, 300.0366, 356.2016],\n",
      "        [145.8066,   0.8140, 342.0980,  45.0438],\n",
      "        [  0.0000,   4.4593, 117.9844, 126.6399]], device='cuda:0'))\n",
      "Analyzing box tensor([327.1812, 268.8648, 359.7600, 328.7418], device='cuda:0') with config parms 288 384\n",
      "Centers are [343.47058 298.80328] and scales are [0.2806732  0.37423098]\n",
      "Analyzing box tensor([134.2380,  74.4081, 240.3535, 204.7526], device='cuda:0') with config parms 288 384\n",
      "Centers are [187.29572 139.58034] and scales are [0.66322196 0.88429594]\n",
      "Analyzing box tensor([ 53.5631,  77.9800, 144.3030, 140.4466], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.93306  109.213295] and scales are [0.5671247  0.75616634]\n",
      "Analyzing box tensor([ 93.5802, 286.6976, 300.0366, 356.2016], device='cuda:0') with config parms 288 384\n",
      "Centers are [196.80841 321.4496 ] and scales are [1.2903526 1.7204702]\n",
      "Analyzing box tensor([145.8066,   0.8140, 342.0980,  45.0438], device='cuda:0') with config parms 288 384\n",
      "Centers are [243.95232   22.928913] and scales are [1.226821  1.6357613]\n",
      "Analyzing box tensor([  0.0000,   4.4593, 117.9844, 126.6399], device='cuda:0') with config parms 288 384\n",
      "Centers are [58.992214 65.54957 ] and scales are [0.7374027 0.9832036]\n",
      "SENDING IN TO POSES [array([343.47058, 298.80328], dtype=float32), array([187.29572, 139.58034], dtype=float32), array([ 98.93306 , 109.213295], dtype=float32), array([196.80841, 321.4496 ], dtype=float32), array([243.95232 ,  22.928913], dtype=float32), array([58.992214, 65.54957 ], dtype=float32)] [array([0.2806732 , 0.37423098], dtype=float32), array([0.66322196, 0.88429594], dtype=float32), array([0.5671247 , 0.75616634], dtype=float32), array([1.2903526, 1.7204702], dtype=float32), array([1.226821 , 1.6357613], dtype=float32), array([0.7374027, 0.9832036], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=7, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[326.8907, 266.2933, 358.7293, 329.8961],\n",
      "        [134.5883,  74.5723, 240.5446, 204.8810],\n",
      "        [ 53.5092,  77.9442, 144.1328, 140.3257],\n",
      "        [144.2847,   0.8160, 342.0253,  44.9614],\n",
      "        [ 89.5116, 286.2497, 303.1884, 356.0137],\n",
      "        [ 79.9700,  76.8810, 146.3015, 122.3621],\n",
      "        [  0.0000,   4.1461, 118.1355, 126.6455]], device='cuda:0')), scores: tensor([0.9509, 0.8344, 0.8000, 0.7180, 0.6872, 0.6251, 0.5311],\n",
      "       device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[326.8907, 266.2933, 358.7293, 329.8961],\n",
      "        [134.5883,  74.5723, 240.5446, 204.8810],\n",
      "        [ 53.5092,  77.9442, 144.1328, 140.3257],\n",
      "        [144.2847,   0.8160, 342.0253,  44.9614],\n",
      "        [ 89.5116, 286.2497, 303.1884, 356.0137],\n",
      "        [ 79.9700,  76.8810, 146.3015, 122.3621],\n",
      "        [  0.0000,   4.1461, 118.1355, 126.6455]], device='cuda:0'))\n",
      "Analyzing box tensor([326.8907, 266.2933, 358.7293, 329.8961], device='cuda:0') with config parms 288 384\n",
      "Centers are [342.81    298.09467] and scales are [0.29813802 0.39751738]\n",
      "Analyzing box tensor([134.5883,  74.5723, 240.5446, 204.8810], device='cuda:0') with config parms 288 384\n",
      "Centers are [187.56645 139.72668] and scales are [0.6622263  0.88296837]\n",
      "Analyzing box tensor([ 53.5092,  77.9442, 144.1328, 140.3257], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.82096 109.13497] and scales are [0.56639755 0.75519675]\n",
      "Analyzing box tensor([144.2847,   0.8160, 342.0253,  44.9614], device='cuda:0') with config parms 288 384\n",
      "Centers are [243.15497   22.888676] and scales are [1.2358787 1.6478384]\n",
      "Analyzing box tensor([ 89.5116, 286.2497, 303.1884, 356.0137], device='cuda:0') with config parms 288 384\n",
      "Centers are [196.34998 321.1317 ] and scales are [1.3354801 1.7806401]\n",
      "Analyzing box tensor([ 79.9700,  76.8810, 146.3015, 122.3621], device='cuda:0') with config parms 288 384\n",
      "Centers are [113.135765  99.62155 ] and scales are [0.41457203 0.55276275]\n",
      "Analyzing box tensor([  0.0000,   4.1461, 118.1355, 126.6455], device='cuda:0') with config parms 288 384\n",
      "Centers are [59.06773  65.395805] and scales are [0.7383466  0.98446214]\n",
      "SENDING IN TO POSES [array([342.81   , 298.09467], dtype=float32), array([187.56645, 139.72668], dtype=float32), array([ 98.82096, 109.13497], dtype=float32), array([243.15497 ,  22.888676], dtype=float32), array([196.34998, 321.1317 ], dtype=float32), array([113.135765,  99.62155 ], dtype=float32), array([59.06773 , 65.395805], dtype=float32)] [array([0.29813802, 0.39751738], dtype=float32), array([0.6622263 , 0.88296837], dtype=float32), array([0.56639755, 0.75519675], dtype=float32), array([1.2358787, 1.6478384], dtype=float32), array([1.3354801, 1.7806401], dtype=float32), array([0.41457203, 0.55276275], dtype=float32), array([0.7383466 , 0.98446214], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[3.2298e+02, 2.6997e+02, 3.5930e+02, 3.3230e+02],\n",
      "        [5.4764e+01, 8.0084e+01, 1.3843e+02, 1.4648e+02],\n",
      "        [1.2303e+02, 7.6581e+01, 2.3726e+02, 2.0890e+02],\n",
      "        [1.9837e-02, 4.2184e+00, 1.1828e+02, 1.2972e+02],\n",
      "        [8.8018e+01, 2.8638e+02, 2.9650e+02, 3.5579e+02],\n",
      "        [1.4177e+02, 7.7669e-01, 3.4275e+02, 4.5694e+01]], device='cuda:0')), scores: tensor([0.9641, 0.8600, 0.8432, 0.6844, 0.6522, 0.5755], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[3.2298e+02, 2.6997e+02, 3.5930e+02, 3.3230e+02],\n",
      "        [5.4764e+01, 8.0084e+01, 1.3843e+02, 1.4648e+02],\n",
      "        [1.2303e+02, 7.6581e+01, 2.3726e+02, 2.0890e+02],\n",
      "        [1.9837e-02, 4.2184e+00, 1.1828e+02, 1.2972e+02],\n",
      "        [8.8018e+01, 2.8638e+02, 2.9650e+02, 3.5579e+02],\n",
      "        [1.4177e+02, 7.7669e-01, 3.4275e+02, 4.5694e+01]], device='cuda:0'))\n",
      "Analyzing box tensor([322.9841, 269.9712, 359.3041, 332.3020], device='cuda:0') with config parms 288 384\n",
      "Centers are [341.14407 301.13657] and scales are [0.29217568 0.38956755]\n",
      "Analyzing box tensor([ 54.7645,  80.0837, 138.4283, 146.4829], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 96.596405 113.28331 ] and scales are [0.5228991 0.6971988]\n",
      "Analyzing box tensor([123.0255,  76.5811, 237.2602, 208.8988], device='cuda:0') with config parms 288 384\n",
      "Centers are [180.14285 142.73996] and scales are [0.713967 0.951956]\n",
      "Analyzing box tensor([1.9837e-02, 4.2184e+00, 1.1828e+02, 1.2972e+02], device='cuda:0') with config parms 288 384\n",
      "Centers are [59.14954 66.96809] and scales are [0.7391213 0.9854951]\n",
      "Analyzing box tensor([ 88.0176, 286.3813, 296.4983, 355.7949], device='cuda:0') with config parms 288 384\n",
      "Centers are [192.25795 321.08807] and scales are [1.3030041 1.737339 ]\n",
      "Analyzing box tensor([141.7671,   0.7767, 342.7517,  45.6935], device='cuda:0') with config parms 288 384\n",
      "Centers are [242.2594    23.235096] and scales are [1.2561542 1.6748722]\n",
      "SENDING IN TO POSES [array([341.14407, 301.13657], dtype=float32), array([ 96.596405, 113.28331 ], dtype=float32), array([180.14285, 142.73996], dtype=float32), array([59.14954, 66.96809], dtype=float32), array([192.25795, 321.08807], dtype=float32), array([242.2594  ,  23.235096], dtype=float32)] [array([0.29217568, 0.38956755], dtype=float32), array([0.5228991, 0.6971988], dtype=float32), array([0.713967, 0.951956], dtype=float32), array([0.7391213, 0.9854951], dtype=float32), array([1.3030041, 1.737339 ], dtype=float32), array([1.2561542, 1.6748722], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[323.9984, 273.1866, 359.6343, 333.4800],\n",
      "        [  1.2180,   1.2785, 115.2580, 135.5789],\n",
      "        [ 92.2434, 288.5818, 287.7645, 355.7087],\n",
      "        [104.8096,  76.4691, 234.3820, 209.5261],\n",
      "        [ 94.6254,  79.9210, 149.8288, 115.4408]], device='cuda:0')), scores: tensor([0.9723, 0.8408, 0.7954, 0.7289, 0.5638], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[323.9984, 273.1866, 359.6343, 333.4800],\n",
      "        [  1.2180,   1.2785, 115.2580, 135.5789],\n",
      "        [ 92.2434, 288.5818, 287.7645, 355.7087],\n",
      "        [104.8096,  76.4691, 234.3820, 209.5261],\n",
      "        [ 94.6254,  79.9210, 149.8288, 115.4408]], device='cuda:0'))\n",
      "Analyzing box tensor([323.9984, 273.1866, 359.6343, 333.4800], device='cuda:0') with config parms 288 384\n",
      "Centers are [341.81635 303.3333 ] and scales are [0.2826256 0.3768341]\n",
      "Analyzing box tensor([  1.2180,   1.2785, 115.2580, 135.5789], device='cuda:0') with config parms 288 384\n",
      "Centers are [58.23803 68.42867] and scales are [0.71274996 0.9503334 ]\n",
      "Analyzing box tensor([ 92.2434, 288.5818, 287.7645, 355.7087], device='cuda:0') with config parms 288 384\n",
      "Centers are [190.00395 322.14526] and scales are [1.2220063 1.6293418]\n",
      "Analyzing box tensor([104.8096,  76.4691, 234.3820, 209.5261], device='cuda:0') with config parms 288 384\n",
      "Centers are [169.59583 142.99762] and scales are [0.8098277 1.0797702]\n",
      "Analyzing box tensor([ 94.6254,  79.9210, 149.8288, 115.4408], device='cuda:0') with config parms 288 384\n",
      "Centers are [122.2271   97.68088] and scales are [0.3450218  0.46002913]\n",
      "SENDING IN TO POSES [array([341.81635, 303.3333 ], dtype=float32), array([58.23803, 68.42867], dtype=float32), array([190.00395, 322.14526], dtype=float32), array([169.59583, 142.99762], dtype=float32), array([122.2271 ,  97.68088], dtype=float32)] [array([0.2826256, 0.3768341], dtype=float32), array([0.71274996, 0.9503334 ], dtype=float32), array([1.2220063, 1.6293418], dtype=float32), array([0.8098277, 1.0797702], dtype=float32), array([0.3450218 , 0.46002913], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[3.2841e+02, 2.7564e+02, 3.5977e+02, 3.3351e+02],\n",
      "        [9.8531e+01, 7.6708e+01, 2.3376e+02, 2.1291e+02],\n",
      "        [9.6054e+01, 2.8866e+02, 2.9148e+02, 3.5529e+02],\n",
      "        [2.5870e-01, 4.0089e+00, 1.1732e+02, 1.5640e+02],\n",
      "        [5.0475e+01, 8.0949e+01, 1.4930e+02, 1.5015e+02]], device='cuda:0')), scores: tensor([0.9843, 0.7755, 0.7532, 0.7199, 0.5722], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[3.2841e+02, 2.7564e+02, 3.5977e+02, 3.3351e+02],\n",
      "        [9.8531e+01, 7.6708e+01, 2.3376e+02, 2.1291e+02],\n",
      "        [9.6054e+01, 2.8866e+02, 2.9148e+02, 3.5529e+02],\n",
      "        [2.5870e-01, 4.0089e+00, 1.1732e+02, 1.5640e+02],\n",
      "        [5.0475e+01, 8.0949e+01, 1.4930e+02, 1.5015e+02]], device='cuda:0'))\n",
      "Analyzing box tensor([328.4146, 275.6444, 359.7731, 333.5105], device='cuda:0') with config parms 288 384\n",
      "Centers are [344.09387 304.57745] and scales are [0.27124757 0.36166343]\n",
      "Analyzing box tensor([ 98.5307,  76.7084, 233.7608, 212.9075], device='cuda:0') with config parms 288 384\n",
      "Centers are [166.14575 144.80795] and scales are [0.8451884 1.1269178]\n",
      "Analyzing box tensor([ 96.0540, 288.6584, 291.4844, 355.2938], device='cuda:0') with config parms 288 384\n",
      "Centers are [193.76921 321.97607] and scales are [1.2214403 1.6285871]\n",
      "Analyzing box tensor([  0.2587,   4.0089, 117.3239, 156.3997], device='cuda:0') with config parms 288 384\n",
      "Centers are [58.79131  80.204315] and scales are [0.7316576 0.9755435]\n",
      "Analyzing box tensor([ 50.4750,  80.9491, 149.3032, 150.1518], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 99.88907  115.550476] and scales are [0.6176763 0.8235686]\n",
      "SENDING IN TO POSES [array([344.09387, 304.57745], dtype=float32), array([166.14575, 144.80795], dtype=float32), array([193.76921, 321.97607], dtype=float32), array([58.79131 , 80.204315], dtype=float32), array([ 99.88907 , 115.550476], dtype=float32)] [array([0.27124757, 0.36166343], dtype=float32), array([0.8451884, 1.1269178], dtype=float32), array([1.2214403, 1.6285871], dtype=float32), array([0.7316576, 0.9755435], dtype=float32), array([0.6176763, 0.8235686], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[3.2311e+02, 2.7676e+02, 3.5968e+02, 3.3326e+02],\n",
      "        [5.5629e+01, 8.1757e+01, 1.4704e+02, 1.5431e+02],\n",
      "        [1.2095e+02, 8.0828e+01, 2.3614e+02, 2.1146e+02],\n",
      "        [9.2655e+01, 2.9002e+02, 2.8899e+02, 3.5529e+02],\n",
      "        [8.6771e-02, 4.8379e+00, 1.1322e+02, 1.4846e+02],\n",
      "        [8.5121e+01, 8.1884e+01, 1.5210e+02, 1.3290e+02]], device='cuda:0')), scores: tensor([0.9795, 0.8938, 0.7979, 0.7014, 0.6689, 0.5265], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[3.2311e+02, 2.7676e+02, 3.5968e+02, 3.3326e+02],\n",
      "        [5.5629e+01, 8.1757e+01, 1.4704e+02, 1.5431e+02],\n",
      "        [1.2095e+02, 8.0828e+01, 2.3614e+02, 2.1146e+02],\n",
      "        [9.2655e+01, 2.9002e+02, 2.8899e+02, 3.5529e+02],\n",
      "        [8.6771e-02, 4.8379e+00, 1.1322e+02, 1.4846e+02],\n",
      "        [8.5121e+01, 8.1884e+01, 1.5210e+02, 1.3290e+02]], device='cuda:0'))\n",
      "Analyzing box tensor([323.1136, 276.7580, 359.6773, 333.2559], device='cuda:0') with config parms 288 384\n",
      "Centers are [341.39545 305.0069 ] and scales are [0.26483387 0.35311186]\n",
      "Analyzing box tensor([ 55.6288,  81.7567, 147.0350, 154.3118], device='cuda:0') with config parms 288 384\n",
      "Centers are [101.33188 118.03427] and scales are [0.57128894 0.76171863]\n",
      "Analyzing box tensor([120.9545,  80.8280, 236.1381, 211.4589], device='cuda:0') with config parms 288 384\n",
      "Centers are [178.5463  146.14343] and scales are [0.7198975 0.9598634]\n",
      "Analyzing box tensor([ 92.6554, 290.0192, 288.9856, 355.2945], device='cuda:0') with config parms 288 384\n",
      "Centers are [190.8205  322.65686] and scales are [1.2270641 1.6360855]\n",
      "Analyzing box tensor([8.6771e-02, 4.8379e+00, 1.1322e+02, 1.4846e+02], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.651405 76.64986 ] and scales are [0.70705783 0.9427439 ]\n",
      "Analyzing box tensor([ 85.1209,  81.8836, 152.1017, 132.8997], device='cuda:0') with config parms 288 384\n",
      "Centers are [118.61128 107.39167] and scales are [0.41862983 0.5581731 ]\n",
      "SENDING IN TO POSES [array([341.39545, 305.0069 ], dtype=float32), array([101.33188, 118.03427], dtype=float32), array([178.5463 , 146.14343], dtype=float32), array([190.8205 , 322.65686], dtype=float32), array([56.651405, 76.64986 ], dtype=float32), array([118.61128, 107.39167], dtype=float32)] [array([0.26483387, 0.35311186], dtype=float32), array([0.57128894, 0.76171863], dtype=float32), array([0.7198975, 0.9598634], dtype=float32), array([1.2270641, 1.6360855], dtype=float32), array([0.70705783, 0.9427439 ], dtype=float32), array([0.41862983, 0.5581731 ], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[3.2254e+02, 2.8262e+02, 3.5970e+02, 3.3321e+02],\n",
      "        [5.3516e+01, 8.2840e+01, 1.5135e+02, 1.5323e+02],\n",
      "        [9.8678e+01, 7.9858e+01, 2.3233e+02, 2.1645e+02],\n",
      "        [8.7015e+01, 2.9179e+02, 2.8094e+02, 3.5544e+02],\n",
      "        [3.5127e-01, 2.3267e+00, 1.1322e+02, 1.4503e+02]], device='cuda:0')), scores: tensor([0.9253, 0.9178, 0.8969, 0.7729, 0.6523], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[3.2254e+02, 2.8262e+02, 3.5970e+02, 3.3321e+02],\n",
      "        [5.3516e+01, 8.2840e+01, 1.5135e+02, 1.5323e+02],\n",
      "        [9.8678e+01, 7.9858e+01, 2.3233e+02, 2.1645e+02],\n",
      "        [8.7015e+01, 2.9179e+02, 2.8094e+02, 3.5544e+02],\n",
      "        [3.5127e-01, 2.3267e+00, 1.1322e+02, 1.4503e+02]], device='cuda:0'))\n",
      "Analyzing box tensor([322.5355, 282.6207, 359.7044, 333.2080], device='cuda:0') with config parms 288 384\n",
      "Centers are [341.11993 307.91437] and scales are [0.237128  0.3161707]\n",
      "Analyzing box tensor([ 53.5157,  82.8403, 151.3474, 153.2262], device='cuda:0') with config parms 288 384\n",
      "Centers are [102.43158 118.03322] and scales are [0.61144817 0.8152643 ]\n",
      "Analyzing box tensor([ 98.6778,  79.8579, 232.3250, 216.4506], device='cuda:0') with config parms 288 384\n",
      "Centers are [165.5014  148.15424] and scales are [0.8352951 1.1137269]\n",
      "Analyzing box tensor([ 87.0146, 291.7947, 280.9432, 355.4364], device='cuda:0') with config parms 288 384\n",
      "Centers are [183.97891 323.6156 ] and scales are [1.2120534 1.6160713]\n",
      "Analyzing box tensor([  0.3513,   2.3267, 113.2224, 145.0307], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.786858 73.67868 ] and scales are [0.7054448 0.9405931]\n",
      "SENDING IN TO POSES [array([341.11993, 307.91437], dtype=float32), array([102.43158, 118.03322], dtype=float32), array([165.5014 , 148.15424], dtype=float32), array([183.97891, 323.6156 ], dtype=float32), array([56.786858, 73.67868 ], dtype=float32)] [array([0.237128 , 0.3161707], dtype=float32), array([0.61144817, 0.8152643 ], dtype=float32), array([0.8352951, 1.1137269], dtype=float32), array([1.2120534, 1.6160713], dtype=float32), array([0.7054448, 0.9405931], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[326.5625, 283.8093, 359.8702, 334.9345],\n",
      "        [ 53.2204,  86.7575, 148.4649, 156.8876],\n",
      "        [106.2571,  82.9174, 233.0136, 216.2184],\n",
      "        [  0.7834,   2.1402, 112.2184, 144.8908]], device='cuda:0')), scores: tensor([0.9761, 0.9120, 0.8060, 0.6642], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[326.5625, 283.8093, 359.8702, 334.9345],\n",
      "        [ 53.2204,  86.7575, 148.4649, 156.8876],\n",
      "        [106.2571,  82.9174, 233.0136, 216.2184],\n",
      "        [  0.7834,   2.1402, 112.2184, 144.8908]], device='cuda:0'))\n",
      "Analyzing box tensor([326.5625, 283.8093, 359.8702, 334.9345], device='cuda:0') with config parms 288 384\n",
      "Centers are [343.21634 309.37195] and scales are [0.23964943 0.31953257]\n",
      "Analyzing box tensor([ 53.2204,  86.7575, 148.4649, 156.8876], device='cuda:0') with config parms 288 384\n",
      "Centers are [100.84262  121.822556] and scales are [0.5952781  0.79370415]\n",
      "Analyzing box tensor([106.2571,  82.9174, 233.0136, 216.2184], device='cuda:0') with config parms 288 384\n",
      "Centers are [169.63533 149.5679 ] and scales are [0.7922281 1.0563041]\n",
      "Analyzing box tensor([  0.7834,   2.1402, 112.2184, 144.8908], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.50091  73.515495] and scales are [0.6964684  0.92862463]\n",
      "SENDING IN TO POSES [array([343.21634, 309.37195], dtype=float32), array([100.84262 , 121.822556], dtype=float32), array([169.63533, 149.5679 ], dtype=float32), array([56.50091 , 73.515495], dtype=float32)] [array([0.23964943, 0.31953257], dtype=float32), array([0.5952781 , 0.79370415], dtype=float32), array([0.7922281, 1.0563041], dtype=float32), array([0.6964684 , 0.92862463], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[324.5884, 283.4965, 359.6494, 340.3013],\n",
      "        [ 56.9330,  88.1303, 146.3770, 161.5249],\n",
      "        [101.3715,  85.6118, 229.7732, 217.1466],\n",
      "        [  1.3448,   6.5390, 111.7569, 135.0507]], device='cuda:0')), scores: tensor([0.9497, 0.9249, 0.8493, 0.5329], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[324.5884, 283.4965, 359.6494, 340.3013],\n",
      "        [ 56.9330,  88.1303, 146.3770, 161.5249],\n",
      "        [101.3715,  85.6118, 229.7732, 217.1466],\n",
      "        [  1.3448,   6.5390, 111.7569, 135.0507]], device='cuda:0'))\n",
      "Analyzing box tensor([324.5884, 283.4965, 359.6494, 340.3013], device='cuda:0') with config parms 288 384\n",
      "Centers are [342.1189 311.8989] and scales are [0.26627254 0.35503003]\n",
      "Analyzing box tensor([ 56.9330,  88.1303, 146.3770, 161.5249], device='cuda:0') with config parms 288 384\n",
      "Centers are [101.655014 124.82761 ] and scales are [0.55902463 0.74536616]\n",
      "Analyzing box tensor([101.3715,  85.6118, 229.7732, 217.1466], device='cuda:0') with config parms 288 384\n",
      "Centers are [165.57236 151.37923] and scales are [0.80251044 1.070014  ]\n",
      "Analyzing box tensor([  1.3448,   6.5390, 111.7569, 135.0507], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.550823 70.794815] and scales are [0.6900759 0.9201013]\n",
      "SENDING IN TO POSES [array([342.1189, 311.8989], dtype=float32), array([101.655014, 124.82761 ], dtype=float32), array([165.57236, 151.37923], dtype=float32), array([56.550823, 70.794815], dtype=float32)] [array([0.26627254, 0.35503003], dtype=float32), array([0.55902463, 0.74536616], dtype=float32), array([0.80251044, 1.070014  ], dtype=float32), array([0.6900759, 0.9201013], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=3, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 59.0435,  87.3911, 151.3548, 167.6759],\n",
      "        [322.0008, 280.6496, 359.4355, 346.6658],\n",
      "        [119.4635,  88.8850, 232.7652, 220.7669]], device='cuda:0')), scores: tensor([0.9658, 0.9008, 0.8433], device='cuda:0'), pred_classes: tensor([0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 59.0435,  87.3911, 151.3548, 167.6759],\n",
      "        [322.0008, 280.6496, 359.4355, 346.6658],\n",
      "        [119.4635,  88.8850, 232.7652, 220.7669]], device='cuda:0'))\n",
      "Analyzing box tensor([ 59.0435,  87.3911, 151.3548, 167.6759], device='cuda:0') with config parms 288 384\n",
      "Centers are [105.19914 127.53349] and scales are [0.576946   0.76926136]\n",
      "Analyzing box tensor([322.0008, 280.6496, 359.4355, 346.6658], device='cuda:0') with config parms 288 384\n",
      "Centers are [340.71814 313.65768] and scales are [0.3094508  0.41260108]\n",
      "Analyzing box tensor([119.4635,  88.8850, 232.7652, 220.7669], device='cuda:0') with config parms 288 384\n",
      "Centers are [176.11438 154.82593] and scales are [0.70813584 0.944181  ]\n",
      "SENDING IN TO POSES [array([105.19914, 127.53349], dtype=float32), array([340.71814, 313.65768], dtype=float32), array([176.11438, 154.82593], dtype=float32)] [array([0.576946  , 0.76926136], dtype=float32), array([0.3094508 , 0.41260108], dtype=float32), array([0.70813584, 0.944181  ], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 54.8235,  90.2258, 153.7881, 172.4607],\n",
      "        [100.4858,  90.1115, 230.4992, 222.1961],\n",
      "        [322.4179, 282.8185, 359.7557, 347.0584],\n",
      "        [  0.7431,   9.5189, 112.7282, 135.9832]], device='cuda:0')), scores: tensor([0.9656, 0.8767, 0.6996, 0.6750], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 54.8235,  90.2258, 153.7881, 172.4607],\n",
      "        [100.4858,  90.1115, 230.4992, 222.1961],\n",
      "        [322.4179, 282.8185, 359.7557, 347.0584],\n",
      "        [  0.7431,   9.5189, 112.7282, 135.9832]], device='cuda:0'))\n",
      "Analyzing box tensor([ 54.8235,  90.2258, 153.7881, 172.4607], device='cuda:0') with config parms 288 384\n",
      "Centers are [104.30583 131.34326] and scales are [0.61852884 0.8247051 ]\n",
      "Analyzing box tensor([100.4858,  90.1115, 230.4992, 222.1961], device='cuda:0') with config parms 288 384\n",
      "Centers are [165.49252 156.15381] and scales are [0.8125835 1.0834447]\n",
      "Analyzing box tensor([322.4179, 282.8185, 359.7557, 347.0584], device='cuda:0') with config parms 288 384\n",
      "Centers are [341.0868  314.93842] and scales are [0.3011248  0.40149975]\n",
      "Analyzing box tensor([  0.7431,   9.5189, 112.7282, 135.9832], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.735672 72.751045] and scales are [0.699907  0.9332094]\n",
      "SENDING IN TO POSES [array([104.30583, 131.34326], dtype=float32), array([165.49252, 156.15381], dtype=float32), array([341.0868 , 314.93842], dtype=float32), array([56.735672, 72.751045], dtype=float32)] [array([0.61852884, 0.8247051 ], dtype=float32), array([0.8125835, 1.0834447], dtype=float32), array([0.3011248 , 0.40149975], dtype=float32), array([0.699907 , 0.9332094], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=7, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 54.6533,  92.6726, 158.2659, 172.9466],\n",
      "        [106.8535,  93.2622, 231.8640, 226.7734],\n",
      "        [321.1105, 291.3694, 359.6772, 346.5411],\n",
      "        [  0.6199,   8.5433, 113.0362, 145.7460],\n",
      "        [257.5912, 265.8821, 335.5682, 341.3035],\n",
      "        [ 79.3308, 242.2046, 286.6393, 351.6557],\n",
      "        [ 53.8589,  93.6644, 255.1407, 323.0676]], device='cuda:0')), scores: tensor([0.9482, 0.8805, 0.7958, 0.7212, 0.6719, 0.5383, 0.5006],\n",
      "       device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 54.6533,  92.6726, 158.2659, 172.9466],\n",
      "        [106.8535,  93.2622, 231.8640, 226.7734],\n",
      "        [321.1105, 291.3694, 359.6772, 346.5411],\n",
      "        [  0.6199,   8.5433, 113.0362, 145.7460],\n",
      "        [257.5912, 265.8821, 335.5682, 341.3035],\n",
      "        [ 79.3308, 242.2046, 286.6393, 351.6557],\n",
      "        [ 53.8589,  93.6644, 255.1407, 323.0676]], device='cuda:0'))\n",
      "Analyzing box tensor([ 54.6533,  92.6726, 158.2659, 172.9466], device='cuda:0') with config parms 288 384\n",
      "Centers are [106.45958 132.80962] and scales are [0.6475788  0.86343837]\n",
      "Analyzing box tensor([106.8535,  93.2622, 231.8640, 226.7734], device='cuda:0') with config parms 288 384\n",
      "Centers are [169.35873 160.01782] and scales are [0.7813152 1.0417536]\n",
      "Analyzing box tensor([321.1105, 291.3694, 359.6772, 346.5411], device='cuda:0') with config parms 288 384\n",
      "Centers are [340.3938  318.95526] and scales are [0.2586173  0.34482306]\n",
      "Analyzing box tensor([  0.6199,   8.5433, 113.0362, 145.7460], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.82805 77.14466] and scales are [0.70260173 0.9368024 ]\n",
      "Analyzing box tensor([257.5912, 265.8821, 335.5682, 341.3035], device='cuda:0') with config parms 288 384\n",
      "Centers are [296.5797  303.59283] and scales are [0.48735577 0.64980775]\n",
      "Analyzing box tensor([ 79.3308, 242.2046, 286.6393, 351.6557], device='cuda:0') with config parms 288 384\n",
      "Centers are [182.98505 296.93015] and scales are [1.2956775 1.7275703]\n",
      "Analyzing box tensor([ 53.8589,  93.6644, 255.1407, 323.0676], device='cuda:0') with config parms 288 384\n",
      "Centers are [154.49982 208.366  ] and scales are [1.2580113 1.6773485]\n",
      "SENDING IN TO POSES [array([106.45958, 132.80962], dtype=float32), array([169.35873, 160.01782], dtype=float32), array([340.3938 , 318.95526], dtype=float32), array([56.82805, 77.14466], dtype=float32), array([296.5797 , 303.59283], dtype=float32), array([182.98505, 296.93015], dtype=float32), array([154.49982, 208.366  ], dtype=float32)] [array([0.6475788 , 0.86343837], dtype=float32), array([0.7813152, 1.0417536], dtype=float32), array([0.2586173 , 0.34482306], dtype=float32), array([0.70260173, 0.9368024 ], dtype=float32), array([0.48735577, 0.64980775], dtype=float32), array([1.2956775, 1.7275703], dtype=float32), array([1.2580113, 1.6773485], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=8, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[3.2127e+02, 2.9255e+02, 3.5974e+02, 3.4895e+02],\n",
      "        [5.2909e+01, 9.6345e+01, 1.5249e+02, 1.7426e+02],\n",
      "        [1.5600e+00, 2.7321e+01, 1.0948e+02, 1.3965e+02],\n",
      "        [1.0505e+02, 9.4929e+01, 2.3103e+02, 2.2894e+02],\n",
      "        [5.9569e+01, 9.4970e+01, 2.7675e+02, 3.3938e+02],\n",
      "        [2.5350e+02, 2.6751e+02, 3.4110e+02, 3.4053e+02],\n",
      "        [2.5694e-01, 9.0880e-01, 6.1196e+01, 6.7303e+01],\n",
      "        [8.1326e+01, 2.4314e+02, 2.8812e+02, 3.5171e+02]], device='cuda:0')), scores: tensor([0.9146, 0.8822, 0.8358, 0.8191, 0.5546, 0.5459, 0.5306, 0.5219],\n",
      "       device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[3.2127e+02, 2.9255e+02, 3.5974e+02, 3.4895e+02],\n",
      "        [5.2909e+01, 9.6345e+01, 1.5249e+02, 1.7426e+02],\n",
      "        [1.5600e+00, 2.7321e+01, 1.0948e+02, 1.3965e+02],\n",
      "        [1.0505e+02, 9.4929e+01, 2.3103e+02, 2.2894e+02],\n",
      "        [5.9569e+01, 9.4970e+01, 2.7675e+02, 3.3938e+02],\n",
      "        [2.5350e+02, 2.6751e+02, 3.4110e+02, 3.4053e+02],\n",
      "        [2.5694e-01, 9.0880e-01, 6.1196e+01, 6.7303e+01],\n",
      "        [8.1326e+01, 2.4314e+02, 2.8812e+02, 3.5171e+02]], device='cuda:0'))\n",
      "Analyzing box tensor([321.2673, 292.5505, 359.7444, 348.9538], device='cuda:0') with config parms 288 384\n",
      "Centers are [340.50586 320.75214] and scales are [0.26439056 0.35252076]\n",
      "Analyzing box tensor([ 52.9090,  96.3450, 152.4861, 174.2561], device='cuda:0') with config parms 288 384\n",
      "Centers are [102.697586 135.30054 ] and scales are [0.622357   0.82980937]\n",
      "Analyzing box tensor([  1.5600,  27.3206, 109.4766, 139.6452], device='cuda:0') with config parms 288 384\n",
      "Centers are [55.51831  83.482895] and scales are [0.6744785 0.8993047]\n",
      "Analyzing box tensor([105.0494,  94.9290, 231.0268, 228.9367], device='cuda:0') with config parms 288 384\n",
      "Centers are [168.03813 161.93283] and scales are [0.7873588 1.0498118]\n",
      "Analyzing box tensor([ 59.5688,  94.9704, 276.7489, 339.3759], device='cuda:0') with config parms 288 384\n",
      "Centers are [168.15884 217.17319] and scales are [1.3573755 1.8098342]\n",
      "Analyzing box tensor([253.4958, 267.5136, 341.1002, 340.5338], device='cuda:0') with config parms 288 384\n",
      "Centers are [297.298  304.0237] and scales are [0.54752743 0.7300365 ]\n",
      "Analyzing box tensor([ 0.2569,  0.9088, 61.1959, 67.3028], device='cuda:0') with config parms 288 384\n",
      "Centers are [30.72641  34.105804] and scales are [0.38086843 0.5078246 ]\n",
      "Analyzing box tensor([ 81.3256, 243.1355, 288.1171, 351.7069], device='cuda:0') with config parms 288 384\n",
      "Centers are [184.72134 297.4212 ] and scales are [1.2924469 1.7232625]\n",
      "SENDING IN TO POSES [array([340.50586, 320.75214], dtype=float32), array([102.697586, 135.30054 ], dtype=float32), array([55.51831 , 83.482895], dtype=float32), array([168.03813, 161.93283], dtype=float32), array([168.15884, 217.17319], dtype=float32), array([297.298 , 304.0237], dtype=float32), array([30.72641 , 34.105804], dtype=float32), array([184.72134, 297.4212 ], dtype=float32)] [array([0.26439056, 0.35252076], dtype=float32), array([0.622357  , 0.82980937], dtype=float32), array([0.6744785, 0.8993047], dtype=float32), array([0.7873588, 1.0498118], dtype=float32), array([1.3573755, 1.8098342], dtype=float32), array([0.54752743, 0.7300365 ], dtype=float32), array([0.38086843, 0.5078246 ], dtype=float32), array([1.2924469, 1.7232625], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 53.8095, 100.2418, 148.1327, 173.1719],\n",
      "        [320.4056, 296.0067, 359.6488, 351.4620],\n",
      "        [115.2979,  96.4737, 231.6451, 228.9924],\n",
      "        [251.6316, 264.5906, 351.3847, 349.7454],\n",
      "        [  1.0279,  17.7474, 108.0523, 146.0761],\n",
      "        [ 64.5841,  97.2411, 285.2202, 342.9411]], device='cuda:0')), scores: tensor([0.8863, 0.7550, 0.7494, 0.7098, 0.6619, 0.6265], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 53.8095, 100.2418, 148.1327, 173.1719],\n",
      "        [320.4056, 296.0067, 359.6488, 351.4620],\n",
      "        [115.2979,  96.4737, 231.6451, 228.9924],\n",
      "        [251.6316, 264.5906, 351.3847, 349.7454],\n",
      "        [  1.0279,  17.7474, 108.0523, 146.0761],\n",
      "        [ 64.5841,  97.2411, 285.2202, 342.9411]], device='cuda:0'))\n",
      "Analyzing box tensor([ 53.8095, 100.2418, 148.1327, 173.1719], device='cuda:0') with config parms 288 384\n",
      "Centers are [100.97107 136.70682] and scales are [0.58952016 0.78602684]\n",
      "Analyzing box tensor([320.4056, 296.0067, 359.6488, 351.4620], device='cuda:0') with config parms 288 384\n",
      "Centers are [340.02716 323.73438] and scales are [0.25994667 0.3465956 ]\n",
      "Analyzing box tensor([115.2979,  96.4737, 231.6451, 228.9924], device='cuda:0') with config parms 288 384\n",
      "Centers are [173.4715  162.73305] and scales are [0.7271698 0.9695598]\n",
      "Analyzing box tensor([251.6316, 264.5906, 351.3847, 349.7454], device='cuda:0') with config parms 288 384\n",
      "Centers are [301.50818 307.168  ] and scales are [0.62345695 0.83127594]\n",
      "Analyzing box tensor([  1.0279,  17.7474, 108.0523, 146.0761], device='cuda:0') with config parms 288 384\n",
      "Centers are [54.540123 81.91173 ] and scales are [0.66890275 0.8918704 ]\n",
      "Analyzing box tensor([ 64.5841,  97.2411, 285.2202, 342.9411], device='cuda:0') with config parms 288 384\n",
      "Centers are [174.90216 220.09106] and scales are [1.3789754 1.8386339]\n",
      "SENDING IN TO POSES [array([100.97107, 136.70682], dtype=float32), array([340.02716, 323.73438], dtype=float32), array([173.4715 , 162.73305], dtype=float32), array([301.50818, 307.168  ], dtype=float32), array([54.540123, 81.91173 ], dtype=float32), array([174.90216, 220.09106], dtype=float32)] [array([0.58952016, 0.78602684], dtype=float32), array([0.25994667, 0.3465956 ], dtype=float32), array([0.7271698, 0.9695598], dtype=float32), array([0.62345695, 0.83127594], dtype=float32), array([0.66890275, 0.8918704 ], dtype=float32), array([1.3789754, 1.8386339], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[253.5325, 265.9122, 341.1240, 350.3618],\n",
      "        [ 59.3119, 101.5424, 289.7473, 349.9810],\n",
      "        [  1.4568,  13.6368, 115.5583, 155.8434],\n",
      "        [ 51.9233, 106.1636, 141.7265, 174.9523],\n",
      "        [321.0576, 298.2757, 359.8633, 353.7123],\n",
      "        [103.6981, 100.8000, 231.1866, 231.3717]], device='cuda:0')), scores: tensor([0.8218, 0.7753, 0.6552, 0.6244, 0.5679, 0.5200], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[253.5325, 265.9122, 341.1240, 350.3618],\n",
      "        [ 59.3119, 101.5424, 289.7473, 349.9810],\n",
      "        [  1.4568,  13.6368, 115.5583, 155.8434],\n",
      "        [ 51.9233, 106.1636, 141.7265, 174.9523],\n",
      "        [321.0576, 298.2757, 359.8633, 353.7123],\n",
      "        [103.6981, 100.8000, 231.1866, 231.3717]], device='cuda:0'))\n",
      "Analyzing box tensor([253.5325, 265.9122, 341.1240, 350.3618], device='cuda:0') with config parms 288 384\n",
      "Centers are [297.32825 308.13696] and scales are [0.547447  0.7299293]\n",
      "Analyzing box tensor([ 59.3119, 101.5424, 289.7473, 349.9810], device='cuda:0') with config parms 288 384\n",
      "Centers are [174.5296  225.76166] and scales are [1.4402218 1.9202957]\n",
      "Analyzing box tensor([  1.4568,  13.6368, 115.5583, 155.8434], device='cuda:0') with config parms 288 384\n",
      "Centers are [58.50756  84.740074] and scales are [0.71313465 0.95084614]\n",
      "Analyzing box tensor([ 51.9233, 106.1636, 141.7265, 174.9523], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 96.82489 140.55794] and scales are [0.56127053 0.7483607 ]\n",
      "Analyzing box tensor([321.0576, 298.2757, 359.8633, 353.7123], device='cuda:0') with config parms 288 384\n",
      "Centers are [340.46048 325.99402] and scales are [0.25985914 0.34647882]\n",
      "Analyzing box tensor([103.6981, 100.8000, 231.1866, 231.3717], device='cuda:0') with config parms 288 384\n",
      "Centers are [167.44232 166.08585] and scales are [0.79680294 1.0624039 ]\n",
      "SENDING IN TO POSES [array([297.32825, 308.13696], dtype=float32), array([174.5296 , 225.76166], dtype=float32), array([58.50756 , 84.740074], dtype=float32), array([ 96.82489, 140.55794], dtype=float32), array([340.46048, 325.99402], dtype=float32), array([167.44232, 166.08585], dtype=float32)] [array([0.547447 , 0.7299293], dtype=float32), array([1.4402218, 1.9202957], dtype=float32), array([0.71313465, 0.95084614], dtype=float32), array([0.56127053, 0.7483607 ], dtype=float32), array([0.25985914, 0.34647882], dtype=float32), array([0.79680294, 1.0624039 ], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[124.6240,  98.6657, 230.6085, 229.8224],\n",
      "        [  0.0000,   9.2481, 118.8297, 154.4527],\n",
      "        [248.2975, 259.9736, 350.7017, 353.6591],\n",
      "        [320.6140, 298.7071, 359.1859, 355.4721],\n",
      "        [ 57.6874, 173.1257, 324.5124, 352.2416],\n",
      "        [ 52.1493, 108.1073, 139.4373, 175.6256]], device='cuda:0')), scores: tensor([0.8129, 0.7521, 0.7498, 0.7196, 0.6292, 0.5468], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[124.6240,  98.6657, 230.6085, 229.8224],\n",
      "        [  0.0000,   9.2481, 118.8297, 154.4527],\n",
      "        [248.2975, 259.9736, 350.7017, 353.6591],\n",
      "        [320.6140, 298.7071, 359.1859, 355.4721],\n",
      "        [ 57.6874, 173.1257, 324.5124, 352.2416],\n",
      "        [ 52.1493, 108.1073, 139.4373, 175.6256]], device='cuda:0'))\n",
      "Analyzing box tensor([124.6240,  98.6657, 230.6085, 229.8224], device='cuda:0') with config parms 288 384\n",
      "Centers are [177.61624 164.24405] and scales are [0.6624031 0.8832043]\n",
      "Analyzing box tensor([  0.0000,   9.2481, 118.8297, 154.4527], device='cuda:0') with config parms 288 384\n",
      "Centers are [59.414845 81.85042 ] and scales are [0.74268556 0.9902474 ]\n",
      "Analyzing box tensor([248.2975, 259.9736, 350.7017, 353.6591], device='cuda:0') with config parms 288 384\n",
      "Centers are [299.4996  306.81635] and scales are [0.6400257  0.85336757]\n",
      "Analyzing box tensor([320.6140, 298.7071, 359.1859, 355.4721], device='cuda:0') with config parms 288 384\n",
      "Centers are [339.89993 327.0896 ] and scales are [0.26608613 0.3547815 ]\n",
      "Analyzing box tensor([ 57.6874, 173.1257, 324.5124, 352.2416], device='cuda:0') with config parms 288 384\n",
      "Centers are [191.09988 262.68365] and scales are [1.6676563 2.2235417]\n",
      "Analyzing box tensor([ 52.1493, 108.1073, 139.4373, 175.6256], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 95.793274 141.86649 ] and scales are [0.5455501 0.7274002]\n",
      "SENDING IN TO POSES [array([177.61624, 164.24405], dtype=float32), array([59.414845, 81.85042 ], dtype=float32), array([299.4996 , 306.81635], dtype=float32), array([339.89993, 327.0896 ], dtype=float32), array([191.09988, 262.68365], dtype=float32), array([ 95.793274, 141.86649 ], dtype=float32)] [array([0.6624031, 0.8832043], dtype=float32), array([0.74268556, 0.9902474 ], dtype=float32), array([0.6400257 , 0.85336757], dtype=float32), array([0.26608613, 0.3547815 ], dtype=float32), array([1.6676563, 2.2235417], dtype=float32), array([0.5455501, 0.7274002], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[115.0816, 102.4962, 232.8541, 229.4322],\n",
      "        [  0.6781,   2.8186, 112.6338, 148.3815],\n",
      "        [ 51.1177, 106.8141, 145.4391, 175.0478],\n",
      "        [320.0113, 302.6463, 359.8545, 355.9251],\n",
      "        [250.6278, 258.7663, 348.6184, 352.9292],\n",
      "        [ 59.1983, 173.3599, 329.4368, 351.5459]], device='cuda:0')), scores: tensor([0.8473, 0.7786, 0.6902, 0.6172, 0.5777, 0.5727], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[115.0816, 102.4962, 232.8541, 229.4322],\n",
      "        [  0.6781,   2.8186, 112.6338, 148.3815],\n",
      "        [ 51.1177, 106.8141, 145.4391, 175.0478],\n",
      "        [320.0113, 302.6463, 359.8545, 355.9251],\n",
      "        [250.6278, 258.7663, 348.6184, 352.9292],\n",
      "        [ 59.1983, 173.3599, 329.4368, 351.5459]], device='cuda:0'))\n",
      "Analyzing box tensor([115.0816, 102.4962, 232.8541, 229.4322], device='cuda:0') with config parms 288 384\n",
      "Centers are [173.96782 165.96416] and scales are [0.7360781 0.9814375]\n",
      "Analyzing box tensor([  0.6781,   2.8186, 112.6338, 148.3815], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.655937 75.60003 ] and scales are [0.69972277 0.9329637 ]\n",
      "Analyzing box tensor([ 51.1177, 106.8141, 145.4391, 175.0478], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.27841 140.93097] and scales are [0.58950835 0.7860112 ]\n",
      "Analyzing box tensor([320.0113, 302.6463, 359.8545, 355.9251], device='cuda:0') with config parms 288 384\n",
      "Centers are [339.93292 329.2857 ] and scales are [0.24974468 0.33299294]\n",
      "Analyzing box tensor([250.6278, 258.7663, 348.6184, 352.9292], device='cuda:0') with config parms 288 384\n",
      "Centers are [299.6231  305.84772] and scales are [0.6124414 0.8165885]\n",
      "Analyzing box tensor([ 59.1983, 173.3599, 329.4368, 351.5459], device='cuda:0') with config parms 288 384\n",
      "Centers are [194.31757 262.4529 ] and scales are [1.6889901 2.251987 ]\n",
      "SENDING IN TO POSES [array([173.96782, 165.96416], dtype=float32), array([56.655937, 75.60003 ], dtype=float32), array([ 98.27841, 140.93097], dtype=float32), array([339.93292, 329.2857 ], dtype=float32), array([299.6231 , 305.84772], dtype=float32), array([194.31757, 262.4529 ], dtype=float32)] [array([0.7360781, 0.9814375], dtype=float32), array([0.69972277, 0.9329637 ], dtype=float32), array([0.58950835, 0.7860112 ], dtype=float32), array([0.24974468, 0.33299294], dtype=float32), array([0.6124414, 0.8165885], dtype=float32), array([1.6889901, 2.251987 ], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 54.1567, 109.4471, 135.5761, 177.8814],\n",
      "        [114.2683, 105.3116, 232.2652, 235.4489],\n",
      "        [247.1149, 249.7515, 353.9951, 356.8291],\n",
      "        [  1.0576,   3.4291, 111.8285, 154.9855],\n",
      "        [ 50.8365, 117.7035, 307.5353, 353.2213]], device='cuda:0')), scores: tensor([0.7653, 0.7545, 0.6233, 0.6044, 0.5868], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 54.1567, 109.4471, 135.5761, 177.8814],\n",
      "        [114.2683, 105.3116, 232.2652, 235.4489],\n",
      "        [247.1149, 249.7515, 353.9951, 356.8291],\n",
      "        [  1.0576,   3.4291, 111.8285, 154.9855],\n",
      "        [ 50.8365, 117.7035, 307.5353, 353.2213]], device='cuda:0'))\n",
      "Analyzing box tensor([ 54.1567, 109.4471, 135.5761, 177.8814], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 94.86636 143.66425] and scales are [0.50887114 0.67849493]\n",
      "Analyzing box tensor([114.2683, 105.3116, 232.2652, 235.4489], device='cuda:0') with config parms 288 384\n",
      "Centers are [173.26672 170.38025] and scales are [0.7374805 0.9833074]\n",
      "Analyzing box tensor([247.1149, 249.7515, 353.9951, 356.8291], device='cuda:0') with config parms 288 384\n",
      "Centers are [300.555   303.29028] and scales are [0.66800094 0.890668  ]\n",
      "Analyzing box tensor([  1.0576,   3.4291, 111.8285, 154.9855], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.44306 79.20733] and scales are [0.7104207 0.9472278]\n",
      "Analyzing box tensor([ 50.8365, 117.7035, 307.5353, 353.2213], device='cuda:0') with config parms 288 384\n",
      "Centers are [179.18591 235.46237] and scales are [1.6043673 2.1391563]\n",
      "SENDING IN TO POSES [array([ 94.86636, 143.66425], dtype=float32), array([173.26672, 170.38025], dtype=float32), array([300.555  , 303.29028], dtype=float32), array([56.44306, 79.20733], dtype=float32), array([179.18591, 235.46237], dtype=float32)] [array([0.50887114, 0.67849493], dtype=float32), array([0.7374805, 0.9833074], dtype=float32), array([0.66800094, 0.890668  ], dtype=float32), array([0.7104207, 0.9472278], dtype=float32), array([1.6043673, 2.1391563], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[115.0339, 103.3561, 231.3290, 238.6790],\n",
      "        [ 52.6539, 106.8142, 144.6519, 177.0256],\n",
      "        [247.4863, 257.4802, 357.3756, 355.2342],\n",
      "        [  0.0000,  10.2785, 107.9340, 169.3580],\n",
      "        [311.6687, 305.2864, 359.3216, 357.2390]], device='cuda:0')), scores: tensor([0.9127, 0.8270, 0.6658, 0.6500, 0.5550], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[115.0339, 103.3561, 231.3290, 238.6790],\n",
      "        [ 52.6539, 106.8142, 144.6519, 177.0256],\n",
      "        [247.4863, 257.4802, 357.3756, 355.2342],\n",
      "        [  0.0000,  10.2785, 107.9340, 169.3580],\n",
      "        [311.6687, 305.2864, 359.3216, 357.2390]], device='cuda:0'))\n",
      "Analyzing box tensor([115.0339, 103.3561, 231.3290, 238.6790], device='cuda:0') with config parms 288 384\n",
      "Centers are [173.18144 171.01755] and scales are [0.7268444  0.96912587]\n",
      "Analyzing box tensor([ 52.6539, 106.8142, 144.6519, 177.0256], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.6529  141.91989] and scales are [0.5749879 0.7666505]\n",
      "Analyzing box tensor([247.4863, 257.4802, 357.3756, 355.2342], device='cuda:0') with config parms 288 384\n",
      "Centers are [302.4309  306.35718] and scales are [0.68680817 0.9157443 ]\n",
      "Analyzing box tensor([  0.0000,  10.2785, 107.9340, 169.3580], device='cuda:0') with config parms 288 384\n",
      "Centers are [53.967007 89.81825 ] and scales are [0.7456848  0.99424636]\n",
      "Analyzing box tensor([311.6687, 305.2864, 359.3216, 357.2390], device='cuda:0') with config parms 288 384\n",
      "Centers are [335.49518 331.26273] and scales are [0.29783058 0.39710745]\n",
      "SENDING IN TO POSES [array([173.18144, 171.01755], dtype=float32), array([ 98.6529 , 141.91989], dtype=float32), array([302.4309 , 306.35718], dtype=float32), array([53.967007, 89.81825 ], dtype=float32), array([335.49518, 331.26273], dtype=float32)] [array([0.7268444 , 0.96912587], dtype=float32), array([0.5749879, 0.7666505], dtype=float32), array([0.68680817, 0.9157443 ], dtype=float32), array([0.7456848 , 0.99424636], dtype=float32), array([0.29783058, 0.39710745], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[114.3600, 101.3903, 230.4809, 241.4129],\n",
      "        [251.8025, 261.9473, 357.4024, 358.3098],\n",
      "        [  0.5470,   8.4327, 107.6935, 165.5995],\n",
      "        [ 49.6146, 108.4515, 149.1186, 179.5036],\n",
      "        [ 48.8206, 104.4280, 252.2555, 351.6990]], device='cuda:0')), scores: tensor([0.8743, 0.8307, 0.7270, 0.6899, 0.6037], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[114.3600, 101.3903, 230.4809, 241.4129],\n",
      "        [251.8025, 261.9473, 357.4024, 358.3098],\n",
      "        [  0.5470,   8.4327, 107.6935, 165.5995],\n",
      "        [ 49.6146, 108.4515, 149.1186, 179.5036],\n",
      "        [ 48.8206, 104.4280, 252.2555, 351.6990]], device='cuda:0'))\n",
      "Analyzing box tensor([114.3600, 101.3903, 230.4809, 241.4129], device='cuda:0') with config parms 288 384\n",
      "Centers are [172.42041 171.40158] and scales are [0.72575545 0.96767396]\n",
      "Analyzing box tensor([251.8025, 261.9473, 357.4024, 358.3098], device='cuda:0') with config parms 288 384\n",
      "Centers are [304.60242 310.12854] and scales are [0.6599995  0.87999934]\n",
      "Analyzing box tensor([  0.5470,   8.4327, 107.6935, 165.5995], device='cuda:0') with config parms 288 384\n",
      "Centers are [54.12024 87.01611] and scales are [0.736719   0.98229206]\n",
      "Analyzing box tensor([ 49.6146, 108.4515, 149.1186, 179.5036], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 99.366585 143.97757 ] and scales are [0.6218999  0.82919985]\n",
      "Analyzing box tensor([ 48.8206, 104.4280, 252.2555, 351.6990], device='cuda:0') with config parms 288 384\n",
      "Centers are [150.53809 228.06349] and scales are [1.2714683 1.695291 ]\n",
      "SENDING IN TO POSES [array([172.42041, 171.40158], dtype=float32), array([304.60242, 310.12854], dtype=float32), array([54.12024, 87.01611], dtype=float32), array([ 99.366585, 143.97757 ], dtype=float32), array([150.53809, 228.06349], dtype=float32)] [array([0.72575545, 0.96767396], dtype=float32), array([0.6599995 , 0.87999934], dtype=float32), array([0.736719  , 0.98229206], dtype=float32), array([0.6218999 , 0.82919985], dtype=float32), array([1.2714683, 1.695291 ], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[113.9907, 104.7947, 231.0616, 243.1022],\n",
      "        [ 53.1235, 110.1960, 149.5182, 184.4580],\n",
      "        [250.3634, 262.1862, 358.1096, 358.4348],\n",
      "        [  0.9545,  17.9547, 108.7358, 154.6933],\n",
      "        [ 50.0567, 107.6156, 281.4535, 350.9814]], device='cuda:0')), scores: tensor([0.8257, 0.8252, 0.7953, 0.7095, 0.6137], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[113.9907, 104.7947, 231.0616, 243.1022],\n",
      "        [ 53.1235, 110.1960, 149.5182, 184.4580],\n",
      "        [250.3634, 262.1862, 358.1096, 358.4348],\n",
      "        [  0.9545,  17.9547, 108.7358, 154.6933],\n",
      "        [ 50.0567, 107.6156, 281.4535, 350.9814]], device='cuda:0'))\n",
      "Analyzing box tensor([113.9907, 104.7947, 231.0616, 243.1022], device='cuda:0') with config parms 288 384\n",
      "Centers are [172.52612 173.94847] and scales are [0.73169315 0.9755909 ]\n",
      "Analyzing box tensor([ 53.1235, 110.1960, 149.5182, 184.4580], device='cuda:0') with config parms 288 384\n",
      "Centers are [101.32088 147.327  ] and scales are [0.60246724 0.8032897 ]\n",
      "Analyzing box tensor([250.3634, 262.1862, 358.1096, 358.4348], device='cuda:0') with config parms 288 384\n",
      "Centers are [304.23648 310.31046] and scales are [0.67341375 0.897885  ]\n",
      "Analyzing box tensor([  0.9545,  17.9547, 108.7358, 154.6933], device='cuda:0') with config parms 288 384\n",
      "Centers are [54.845177 86.324005] and scales are [0.67363334 0.8981779 ]\n",
      "Analyzing box tensor([ 50.0567, 107.6156, 281.4535, 350.9814], device='cuda:0') with config parms 288 384\n",
      "Centers are [165.75508 229.2985 ] and scales are [1.4462298 1.9283066]\n",
      "SENDING IN TO POSES [array([172.52612, 173.94847], dtype=float32), array([101.32088, 147.327  ], dtype=float32), array([304.23648, 310.31046], dtype=float32), array([54.845177, 86.324005], dtype=float32), array([165.75508, 229.2985 ], dtype=float32)] [array([0.73169315, 0.9755909 ], dtype=float32), array([0.60246724, 0.8032897 ], dtype=float32), array([0.67341375, 0.897885  ], dtype=float32), array([0.67363334, 0.8981779 ], dtype=float32), array([1.4462298, 1.9283066], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 46.6760, 114.1131, 150.1513, 184.3135],\n",
      "        [116.2874, 106.8290, 229.9433, 240.2035],\n",
      "        [243.6368, 261.1933, 354.2955, 355.1373],\n",
      "        [  1.3179,   6.4200, 111.7981, 181.1491],\n",
      "        [ 47.4753, 108.3192, 253.1618, 353.2555]], device='cuda:0')), scores: tensor([0.8816, 0.7946, 0.7450, 0.7432, 0.6209], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 46.6760, 114.1131, 150.1513, 184.3135],\n",
      "        [116.2874, 106.8290, 229.9433, 240.2035],\n",
      "        [243.6368, 261.1933, 354.2955, 355.1373],\n",
      "        [  1.3179,   6.4200, 111.7981, 181.1491],\n",
      "        [ 47.4753, 108.3192, 253.1618, 353.2555]], device='cuda:0'))\n",
      "Analyzing box tensor([ 46.6760, 114.1131, 150.1513, 184.3135], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.41367 149.21329] and scales are [0.6467204 0.8622939]\n",
      "Analyzing box tensor([116.2874, 106.8290, 229.9433, 240.2035], device='cuda:0') with config parms 288 384\n",
      "Centers are [173.11536 173.51627] and scales are [0.7103494  0.94713247]\n",
      "Analyzing box tensor([243.6368, 261.1933, 354.2955, 355.1373], device='cuda:0') with config parms 288 384\n",
      "Centers are [298.96616 308.1653 ] and scales are [0.69161683 0.92215574]\n",
      "Analyzing box tensor([  1.3179,   6.4200, 111.7981, 181.1491], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.558025 93.78458 ] and scales are [0.81904256 1.0920568 ]\n",
      "Analyzing box tensor([ 47.4753, 108.3192, 253.1618, 353.2555], device='cuda:0') with config parms 288 384\n",
      "Centers are [150.31856 230.78735] and scales are [1.2855406 1.7140542]\n",
      "SENDING IN TO POSES [array([ 98.41367, 149.21329], dtype=float32), array([173.11536, 173.51627], dtype=float32), array([298.96616, 308.1653 ], dtype=float32), array([56.558025, 93.78458 ], dtype=float32), array([150.31856, 230.78735], dtype=float32)] [array([0.6467204, 0.8622939], dtype=float32), array([0.7103494 , 0.94713247], dtype=float32), array([0.69161683, 0.92215574], dtype=float32), array([0.81904256, 1.0920568 ], dtype=float32), array([1.2855406, 1.7140542], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[122.0113, 103.8839, 229.0006, 237.9328],\n",
      "        [ 52.1372, 118.6228, 149.8288, 185.3336],\n",
      "        [242.8730, 258.4668, 357.7705, 357.0821],\n",
      "        [  0.0000,   1.3998, 117.7993, 170.8705]], device='cuda:0')), scores: tensor([0.9532, 0.8845, 0.8456, 0.7208], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[122.0113, 103.8839, 229.0006, 237.9328],\n",
      "        [ 52.1372, 118.6228, 149.8288, 185.3336],\n",
      "        [242.8730, 258.4668, 357.7705, 357.0821],\n",
      "        [  0.0000,   1.3998, 117.7993, 170.8705]], device='cuda:0'))\n",
      "Analyzing box tensor([122.0113, 103.8839, 229.0006, 237.9328], device='cuda:0') with config parms 288 384\n",
      "Centers are [175.50595 170.90837] and scales are [0.66868335 0.89157784]\n",
      "Analyzing box tensor([ 52.1372, 118.6228, 149.8288, 185.3336], device='cuda:0') with config parms 288 384\n",
      "Centers are [100.983   151.97815] and scales are [0.610572   0.81409603]\n",
      "Analyzing box tensor([242.8730, 258.4668, 357.7705, 357.0821], device='cuda:0') with config parms 288 384\n",
      "Centers are [300.32172 307.7744 ] and scales are [0.7181093 0.9574791]\n",
      "Analyzing box tensor([  0.0000,   1.3998, 117.7993, 170.8705], device='cuda:0') with config parms 288 384\n",
      "Centers are [58.899666 86.13515 ] and scales are [0.79439414 1.0591923 ]\n",
      "SENDING IN TO POSES [array([175.50595, 170.90837], dtype=float32), array([100.983  , 151.97815], dtype=float32), array([300.32172, 307.7744 ], dtype=float32), array([58.899666, 86.13515 ], dtype=float32)] [array([0.66868335, 0.89157784], dtype=float32), array([0.610572  , 0.81409603], dtype=float32), array([0.7181093, 0.9574791], dtype=float32), array([0.79439414, 1.0591923 ], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 52.0708, 122.6984, 144.3179, 186.7546],\n",
      "        [243.4404, 252.0259, 354.8422, 358.2476],\n",
      "        [  1.4716,   7.9449, 111.0822, 165.7202],\n",
      "        [114.8084, 105.6581, 228.8776, 242.9950],\n",
      "        [ 50.2174, 103.3565, 245.0961, 347.2128]], device='cuda:0')), scores: tensor([0.8901, 0.8409, 0.8378, 0.8370, 0.7302], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 52.0708, 122.6984, 144.3179, 186.7546],\n",
      "        [243.4404, 252.0259, 354.8422, 358.2476],\n",
      "        [  1.4716,   7.9449, 111.0822, 165.7202],\n",
      "        [114.8084, 105.6581, 228.8776, 242.9950],\n",
      "        [ 50.2174, 103.3565, 245.0961, 347.2128]], device='cuda:0'))\n",
      "Analyzing box tensor([ 52.0708, 122.6984, 144.3179, 186.7546], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.19434 154.72652] and scales are [0.57654464 0.7687262 ]\n",
      "Analyzing box tensor([243.4404, 252.0259, 354.8422, 358.2476], device='cuda:0') with config parms 288 384\n",
      "Centers are [299.1413  305.13675] and scales are [0.69626147 0.92834866]\n",
      "Analyzing box tensor([  1.4716,   7.9449, 111.0822, 165.7202], device='cuda:0') with config parms 288 384\n",
      "Centers are [56.27691 86.83256] and scales are [0.7395721 0.9860962]\n",
      "Analyzing box tensor([114.8084, 105.6581, 228.8776, 242.9950], device='cuda:0') with config parms 288 384\n",
      "Centers are [171.843   174.32657] and scales are [0.712932 0.950576]\n",
      "Analyzing box tensor([ 50.2174, 103.3565, 245.0961, 347.2128], device='cuda:0') with config parms 288 384\n",
      "Centers are [147.65675 225.2847 ] and scales are [1.2179914 1.6239884]\n",
      "SENDING IN TO POSES [array([ 98.19434, 154.72652], dtype=float32), array([299.1413 , 305.13675], dtype=float32), array([56.27691, 86.83256], dtype=float32), array([171.843  , 174.32657], dtype=float32), array([147.65675, 225.2847 ], dtype=float32)] [array([0.57654464, 0.7687262 ], dtype=float32), array([0.69626147, 0.92834866], dtype=float32), array([0.7395721, 0.9860962], dtype=float32), array([0.712932, 0.950576], dtype=float32), array([1.2179914, 1.6239884], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[110.9491, 104.8601, 226.5010, 244.2011],\n",
      "        [ 50.4373, 118.7671, 146.5807, 184.9799],\n",
      "        [239.6358, 248.3078, 355.6671, 354.0667],\n",
      "        [ 41.5642, 107.5913, 242.4071, 352.5788],\n",
      "        [  1.3364,   8.9617, 108.9210, 184.9199]], device='cuda:0')), scores: tensor([0.8322, 0.8203, 0.7533, 0.6880, 0.6877], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[110.9491, 104.8601, 226.5010, 244.2011],\n",
      "        [ 50.4373, 118.7671, 146.5807, 184.9799],\n",
      "        [239.6358, 248.3078, 355.6671, 354.0667],\n",
      "        [ 41.5642, 107.5913, 242.4071, 352.5788],\n",
      "        [  1.3364,   8.9617, 108.9210, 184.9199]], device='cuda:0'))\n",
      "Analyzing box tensor([110.9491, 104.8601, 226.5010, 244.2011], device='cuda:0') with config parms 288 384\n",
      "Centers are [168.72507 174.53061] and scales are [0.7221993 0.9629324]\n",
      "Analyzing box tensor([ 50.4373, 118.7671, 146.5807, 184.9799], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 98.509   151.87352] and scales are [0.6008966 0.8011955]\n",
      "Analyzing box tensor([239.6358, 248.3078, 355.6671, 354.0667], device='cuda:0') with config parms 288 384\n",
      "Centers are [297.65143 301.18726] and scales are [0.72519577 0.9669277 ]\n",
      "Analyzing box tensor([ 41.5642, 107.5913, 242.4071, 352.5788], device='cuda:0') with config parms 288 384\n",
      "Centers are [141.98561 230.08505] and scales are [1.255268  1.6736908]\n",
      "Analyzing box tensor([  1.3364,   8.9617, 108.9210, 184.9199], device='cuda:0') with config parms 288 384\n",
      "Centers are [55.12873 96.94081] and scales are [0.8248042 1.099739 ]\n",
      "SENDING IN TO POSES [array([168.72507, 174.53061], dtype=float32), array([ 98.509  , 151.87352], dtype=float32), array([297.65143, 301.18726], dtype=float32), array([141.98561, 230.08505], dtype=float32), array([55.12873, 96.94081], dtype=float32)] [array([0.7221993, 0.9629324], dtype=float32), array([0.6008966, 0.8011955], dtype=float32), array([0.72519577, 0.9669277 ], dtype=float32), array([1.255268 , 1.6736908], dtype=float32), array([0.8248042, 1.099739 ], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[235.7392, 253.2817, 355.1985, 357.1808],\n",
      "        [ 41.3350, 107.3162, 236.0205, 353.2734],\n",
      "        [109.1304, 105.7099, 224.6971, 245.4709],\n",
      "        [  1.0924,  12.0035, 132.8654, 205.7899]], device='cuda:0')), scores: tensor([0.8397, 0.7497, 0.7291, 0.6843], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[235.7392, 253.2817, 355.1985, 357.1808],\n",
      "        [ 41.3350, 107.3162, 236.0205, 353.2734],\n",
      "        [109.1304, 105.7099, 224.6971, 245.4709],\n",
      "        [  1.0924,  12.0035, 132.8654, 205.7899]], device='cuda:0'))\n",
      "Analyzing box tensor([235.7392, 253.2817, 355.1985, 357.1808], device='cuda:0') with config parms 288 384\n",
      "Centers are [295.46884 305.23126] and scales are [0.74662083 0.9954944 ]\n",
      "Analyzing box tensor([ 41.3350, 107.3162, 236.0205, 353.2734], device='cuda:0') with config parms 288 384\n",
      "Centers are [138.67773 230.29478] and scales are [1.216784  1.6223788]\n",
      "Analyzing box tensor([109.1304, 105.7099, 224.6971, 245.4709], device='cuda:0') with config parms 288 384\n",
      "Centers are [166.91379 175.59042] and scales are [0.7222918 0.9630558]\n",
      "Analyzing box tensor([  1.0924,  12.0035, 132.8654, 205.7899], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 66.97886  108.896675] and scales are [0.9083737 1.211165 ]\n",
      "SENDING IN TO POSES [array([295.46884, 305.23126], dtype=float32), array([138.67773, 230.29478], dtype=float32), array([166.91379, 175.59042], dtype=float32), array([ 66.97886 , 108.896675], dtype=float32)] [array([0.74662083, 0.9954944 ], dtype=float32), array([1.216784 , 1.6223788], dtype=float32), array([0.7222918, 0.9630558], dtype=float32), array([0.9083737, 1.211165 ], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=4, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[ 43.4381, 104.6407, 237.3872, 344.1425],\n",
      "        [235.7400, 257.3215, 355.9620, 356.3910],\n",
      "        [ 99.4705, 105.6592, 223.5464, 246.9020],\n",
      "        [  0.8550,   9.3906, 150.8408, 270.9344]], device='cuda:0')), scores: tensor([0.8044, 0.7978, 0.6792, 0.6419], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[ 43.4381, 104.6407, 237.3872, 344.1425],\n",
      "        [235.7400, 257.3215, 355.9620, 356.3910],\n",
      "        [ 99.4705, 105.6592, 223.5464, 246.9020],\n",
      "        [  0.8550,   9.3906, 150.8408, 270.9344]], device='cuda:0'))\n",
      "Analyzing box tensor([ 43.4381, 104.6407, 237.3872, 344.1425], device='cuda:0') with config parms 288 384\n",
      "Centers are [140.41269 224.3916 ] and scales are [1.2121818 1.6162425]\n",
      "Analyzing box tensor([235.7400, 257.3215, 355.9620, 356.3910], device='cuda:0') with config parms 288 384\n",
      "Centers are [295.851   306.85626] and scales are [0.75138783 1.0018505 ]\n",
      "Analyzing box tensor([ 99.4705, 105.6592, 223.5464, 246.9020], device='cuda:0') with config parms 288 384\n",
      "Centers are [161.50845 176.28058] and scales are [0.7754741 1.0339655]\n",
      "Analyzing box tensor([  0.8550,   9.3906, 150.8408, 270.9344], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 75.84787 140.16254] and scales are [1.2259865 1.6346488]\n",
      "SENDING IN TO POSES [array([140.41269, 224.3916 ], dtype=float32), array([295.851  , 306.85626], dtype=float32), array([161.50845, 176.28058], dtype=float32), array([ 75.84787, 140.16254], dtype=float32)] [array([1.2121818, 1.6162425], dtype=float32), array([0.75138783, 1.0018505 ], dtype=float32), array([0.7754741, 1.0339655], dtype=float32), array([1.2259865, 1.6346488], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=5, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[101.1284, 106.3228, 223.2823, 245.0762],\n",
      "        [ 45.9471, 120.0277, 140.8868, 184.2836],\n",
      "        [ 42.4989, 118.8464, 249.5173, 353.0280],\n",
      "        [236.7929, 253.4959, 355.5015, 356.4100],\n",
      "        [  1.0764,  14.5715, 105.5246, 214.2154]], device='cuda:0')), scores: tensor([0.8136, 0.8025, 0.7653, 0.7486, 0.5819], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[101.1284, 106.3228, 223.2823, 245.0762],\n",
      "        [ 45.9471, 120.0277, 140.8868, 184.2836],\n",
      "        [ 42.4989, 118.8464, 249.5173, 353.0280],\n",
      "        [236.7929, 253.4959, 355.5015, 356.4100],\n",
      "        [  1.0764,  14.5715, 105.5246, 214.2154]], device='cuda:0'))\n",
      "Analyzing box tensor([101.1284, 106.3228, 223.2823, 245.0762], device='cuda:0') with config parms 288 384\n",
      "Centers are [162.20535 175.6995 ] and scales are [0.7634616 1.0179487]\n",
      "Analyzing box tensor([ 45.9471, 120.0277, 140.8868, 184.2836], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 93.41695 152.15569] and scales are [0.5933728 0.7911638]\n",
      "Analyzing box tensor([ 42.4989, 118.8464, 249.5173, 353.0280], device='cuda:0') with config parms 288 384\n",
      "Centers are [146.00809 235.93716] and scales are [1.2938652 1.7251537]\n",
      "Analyzing box tensor([236.7929, 253.4959, 355.5015, 356.4100], device='cuda:0') with config parms 288 384\n",
      "Centers are [296.14722 304.95294] and scales are [0.7419287  0.98923826]\n",
      "Analyzing box tensor([  1.0764,  14.5715, 105.5246, 214.2154], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 53.30051  114.393425] and scales are [0.93583095 1.2477746 ]\n",
      "SENDING IN TO POSES [array([162.20535, 175.6995 ], dtype=float32), array([ 93.41695, 152.15569], dtype=float32), array([146.00809, 235.93716], dtype=float32), array([296.14722, 304.95294], dtype=float32), array([ 53.30051 , 114.393425], dtype=float32)] [array([0.7634616, 1.0179487], dtype=float32), array([0.5933728, 0.7911638], dtype=float32), array([1.2938652, 1.7251537], dtype=float32), array([0.7419287 , 0.98923826], dtype=float32), array([0.93583095, 1.2477746 ], dtype=float32)]\n",
      "Passed box_model an geimg, received predictions Instances(num_instances=6, image_height=360, image_width=360, fields=[pred_boxes: Boxes(tensor([[234.5869, 258.3650, 350.9219, 356.3656],\n",
      "        [101.7167, 105.7350, 223.8868, 248.3119],\n",
      "        [ 38.9105, 107.9737, 224.6131, 355.5455],\n",
      "        [ 35.6860, 118.5358, 141.0735, 184.3081],\n",
      "        [  1.2435,   7.1062, 139.1661, 266.4929],\n",
      "        [  1.3456,   0.0000,  46.7552,  81.1788]], device='cuda:0')), scores: tensor([0.8422, 0.8269, 0.7875, 0.7238, 0.6853, 0.5038], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0], device='cuda:0')])\n",
      "Corresponding pred_boxes are Boxes(tensor([[234.5869, 258.3650, 350.9219, 356.3656],\n",
      "        [101.7167, 105.7350, 223.8868, 248.3119],\n",
      "        [ 38.9105, 107.9737, 224.6131, 355.5455],\n",
      "        [ 35.6860, 118.5358, 141.0735, 184.3081],\n",
      "        [  1.2435,   7.1062, 139.1661, 266.4929],\n",
      "        [  1.3456,   0.0000,  46.7552,  81.1788]], device='cuda:0'))\n",
      "Analyzing box tensor([234.5869, 258.3650, 350.9219, 356.3656], device='cuda:0') with config parms 288 384\n",
      "Centers are [292.7544 307.3653] and scales are [0.7270937 0.9694583]\n",
      "Analyzing box tensor([101.7167, 105.7350, 223.8868, 248.3119], device='cuda:0') with config parms 288 384\n",
      "Centers are [162.80173 177.02344] and scales are [0.76356316 1.0180842 ]\n",
      "Analyzing box tensor([ 38.9105, 107.9737, 224.6131, 355.5455], device='cuda:0') with config parms 288 384\n",
      "Centers are [131.76184 231.75958] and scales are [1.1606412 1.5475218]\n",
      "Analyzing box tensor([ 35.6860, 118.5358, 141.0735, 184.3081], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 88.379745 151.42194 ] and scales are [0.6586714  0.87822855]\n",
      "Analyzing box tensor([  1.2435,   7.1062, 139.1661, 266.4929], device='cuda:0') with config parms 288 384\n",
      "Centers are [ 70.2048  136.79958] and scales are [1.2158749 1.6211666]\n",
      "Analyzing box tensor([ 1.3456,  0.0000, 46.7552, 81.1788], device='cuda:0') with config parms 288 384\n",
      "Centers are [24.050385 40.589386] and scales are [0.38052547 0.5073673 ]\n",
      "SENDING IN TO POSES [array([292.7544, 307.3653], dtype=float32), array([162.80173, 177.02344], dtype=float32), array([131.76184, 231.75958], dtype=float32), array([ 88.379745, 151.42194 ], dtype=float32), array([ 70.2048 , 136.79958], dtype=float32), array([24.050385, 40.589386], dtype=float32)] [array([0.7270937, 0.9694583], dtype=float32), array([0.76356316, 1.0180842 ], dtype=float32), array([1.1606412, 1.5475218], dtype=float32), array([0.6586714 , 0.87822855], dtype=float32), array([1.2158749, 1.6211666], dtype=float32), array([0.38052547, 0.5073673 ], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Inference on Frame Number  32\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0f9686d75160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pwd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0f9686d75160>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mimage_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mimage_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Passed box_model an geimg, received predictions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"height\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"width\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \"\"\"\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetected_instances\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;34m\"proposals\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mpred_objectness_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_anchor_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0;31m# Transpose the Hi*Wi*A dimension to the middle:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         pred_objectness_logits = [\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/detectron2/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mpred_anchor_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mpred_objectness_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectness_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mpred_anchor_deltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/multitaskenv/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# modified from original source by Krishna Patel\n",
    "# source: https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/demo/inference.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"We are using torch version\", torch.__version__)\n",
    "print(\"We are using torchvision version\", torchvision.__version__)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./deep-high-resolution-net.pytorch/lib\")\n",
    "import time\n",
    "\n",
    "!pwd\n",
    "from models import pose_hrnet\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from core.inference import get_final_preds\n",
    "from utils.transforms import get_affine_transform\n",
    "\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "\n",
    "CTX = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "_BLACK = (0, 0, 0)\n",
    "_RED = (0, 0, 255)\n",
    "_BLUE = (255, 0, 0) \n",
    "_PURPLE = (204, 0, 153)\n",
    "_ORANGE = (51, 153, 255)\n",
    "_LBROWN = (0, 153, 230)\n",
    "keypoint_colors = { '1': _RED, '2': _RED, '3': _RED, '4': _RED, '5': _RED,\n",
    "                            '6': _ORANGE, '7': _ORANGE, '8': _ORANGE, '9': _ORANGE, \n",
    "                            '10': _LBROWN, '11': _LBROWN, '12': _LBROWN, '13': _LBROWN,\n",
    "                            '14': _BLUE, '15': _BLUE, '16': _BLUE, '17': _BLUE,\n",
    "                            '18': _PURPLE, '19': _PURPLE, '20': _PURPLE, '21': _PURPLE\n",
    "                            }\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'hand',\n",
    "]\n",
    "\n",
    "\n",
    "def get_person_detection_boxes(model, img, threshold=0.5):\n",
    "    pil_image = Image.fromarray(img)  # Load the image\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  # Defing PyTorch Transform\n",
    "    transformed_img = transform(pil_image)  # Apply the transform to the image\n",
    "    pred = model([transformed_img.to(CTX)])  # Pass the image to the model\n",
    "\n",
    "    # Use the first detected person\n",
    "    pred_classes = [COCO_INSTANCE_CATEGORY_NAMES[i]\n",
    "                    for i in list(pred[0]['labels'].cpu().numpy())]  # Get the Prediction Score\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])]\n",
    "                  for i in list(pred[0]['boxes'].cpu().detach().numpy())]  # Bounding boxes\n",
    "    pred_scores = list(pred[0]['scores'].cpu().detach().numpy())\n",
    "\n",
    "    person_boxes = []\n",
    "    # Select box has score larger than threshold and is person\n",
    "    for pred_class, pred_box, pred_score in zip(pred_classes, pred_boxes, pred_scores):\n",
    "        if (pred_score > threshold) and pred_class == 'hand':\n",
    "            person_boxes.append(pred_box)\n",
    "\n",
    "    return person_boxes\n",
    "\n",
    "\n",
    "def get_pose_estimation_prediction(pose_model, image, centers, scales, transform):\n",
    "    rotation = 0\n",
    "\n",
    "    # pose estimation transformation\n",
    "    model_inputs = []\n",
    "    for center, scale in zip(centers, scales):\n",
    "        trans = get_affine_transform(center, scale, rotation, cfg.MODEL.IMAGE_SIZE)\n",
    "        # Crop smaller image of people\n",
    "        model_input = cv2.warpAffine(\n",
    "            image,\n",
    "            trans,\n",
    "            (int(cfg.MODEL.IMAGE_SIZE[0]), int(cfg.MODEL.IMAGE_SIZE[1])),\n",
    "            flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        # hwc -> 1chw\n",
    "        model_input = transform(model_input)#.unsqueeze(0)\n",
    "        model_inputs.append(model_input)\n",
    "\n",
    "    # n * 1chw -> nchw\n",
    "    model_inputs = torch.stack(model_inputs)\n",
    "\n",
    "    # compute output heatmap\n",
    "    output = pose_model(model_inputs.to(CTX))\n",
    "    coords, _ = get_final_preds(\n",
    "        cfg,\n",
    "        output.cpu().detach().numpy(),\n",
    "        np.asarray(centers),\n",
    "        np.asarray(scales))\n",
    "\n",
    "    return coords\n",
    "\n",
    "\n",
    "def box_to_center_scale(box, model_image_width, model_image_height):\n",
    "    \"\"\"convert a box to center,scale information required for pose transformation\n",
    "    Parameters\n",
    "    ----------\n",
    "    box : list of tuple\n",
    "        list of length 2 with two tuples of floats representing\n",
    "        bottom left and top right corner of a box\n",
    "    model_image_width : int\n",
    "    model_image_height : int\n",
    "    Returns\n",
    "    -------\n",
    "    (numpy array, numpy array)\n",
    "        Two numpy arrays, coordinates for the center of the box and the scale of the box\n",
    "    \"\"\"\n",
    "    center = np.zeros((2), dtype=np.float32)\n",
    "\n",
    "    x1, y1, x2, y2 = box\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "    center[0] = x1 + box_width * 0.5\n",
    "    center[1] = y1 + box_height * 0.5\n",
    "\n",
    "    aspect_ratio = model_image_width * 1.0 / model_image_height\n",
    "    pixel_std = 200\n",
    "\n",
    "    if box_width > aspect_ratio * box_height:\n",
    "        box_height = box_width * 1.0 / aspect_ratio\n",
    "    elif box_width < aspect_ratio * box_height:\n",
    "        box_width = box_height * aspect_ratio\n",
    "    scale = np.array(\n",
    "        [box_width * 1.0 / pixel_std, box_height * 1.0 / pixel_std],\n",
    "        dtype=np.float32)\n",
    "    if center[0] != -1:\n",
    "        scale = scale * 1.25\n",
    "\n",
    "    return center, scale\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Surgery Hand and Keypoint Detection on Video')\n",
    "    parser.add_argument('--cfg', type=str, default=\"keypoints.yaml\")\n",
    "    parser.add_argument('--bb_cfg', type=str, default=\"bbox.yaml\")\n",
    "    parser.add_argument('--video', type=str, default = \"slap.mp4\")\n",
    "    parser.add_argument('--produce_vid', action='store_true')\n",
    "    parser.add_argument('--out_json', type=str, default='out.json')\n",
    "    parser.add_argument('--tracking', action='store_true')\n",
    "    parser.add_argument('opts',\n",
    "                        help='Modify config options using the command-line',\n",
    "                        default=None,\n",
    "                        nargs=argparse.REMAINDER)\n",
    "\n",
    "    args = parser.parse_args(\"--produce_vid\".split())\n",
    "    # args expected by supporting codebase\n",
    "    args.modelDir = ''\n",
    "    args.logDir = ''\n",
    "    args.dataDir = ''\n",
    "    args.prevModelDir = ''\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    # transformation\n",
    "    pose_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # cudnn related setting\n",
    "    cudnn.benchmark = cfg.CUDNN.BENCHMARK\n",
    "    torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n",
    "    torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n",
    "\n",
    "    args = parse_args()\n",
    "    update_config(cfg, args)\n",
    "\n",
    "    bbox_cfg = get_cfg()\n",
    "    bbox_cfg.merge_from_file(args.bb_cfg)\n",
    "    box_model = DefaultPredictor(bbox_cfg)\n",
    "\n",
    "    print(\"Getting pose model from\", cfg.MODEL.NAME+'.get_pose_net')\n",
    "    print(\"Using cfg\", cfg)\n",
    "    pose_model = eval(cfg.MODEL.NAME+'.get_pose_net')(\n",
    "        cfg, is_train=False\n",
    "    )\n",
    "\n",
    "    if cfg.TEST.MODEL_FILE:\n",
    "        print('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "        pose_model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=False)\n",
    "    else:\n",
    "        print('expected model defined in config at TEST.MODEL_FILE')\n",
    "\n",
    "    pose_model.to(CTX)\n",
    "    pose_model.eval()\n",
    "\n",
    "    print(\"Opening\", args.video)\n",
    "    video = cv2.VideoCapture(args.video)\n",
    "    width = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    data = {}\n",
    "\n",
    "    \n",
    "    print(\"We are going to produce the video\", args.produce_vid)\n",
    "    if args.produce_vid:\n",
    "        fourcc = cv2.VideoWriter_fourcc('F', 'M', 'P', '4')\n",
    "        video_tracked = cv2.VideoWriter('predictions.mp4', fourcc, fps, (int(width), int(height)))\n",
    "\n",
    "    frame_num = 0\n",
    "    while video.isOpened():    \n",
    "        print(\"Performing Inference on Frame Number \", frame_num, end='\\r')\n",
    "        _, frame = video.read()\n",
    "        if frame is None or frame.size == 0:\n",
    "            break\n",
    "\n",
    "        img = frame\n",
    "        if args.produce_vid:\n",
    "            image_debug = img.copy()\n",
    "        image_pose = img.copy()\n",
    "        predictions = box_model(img)['instances']\n",
    "        \n",
    "        print(\"Passed box_model an geimg, received predictions\", predictions)\n",
    "        #produces Prediction boxes in the form of (x1, y1, x2, y2) where x measured from left and y measured from top\n",
    "        #these boxes also come with scorse and class annotations\n",
    "        \n",
    "        pred_boxes = predictions.pred_boxes\n",
    "        print(\"Corresponding pred_boxes are\", pred_boxes)\n",
    "\n",
    "        centers = []\n",
    "        scales = []\n",
    "        for box in pred_boxes:\n",
    "            #for each box, returns the center of the box and the scale of the box relative to the input image\n",
    "            print(\"Analyzing box\", box, \"with config parms\", cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1])\n",
    "            center, scale = box_to_center_scale(box, cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1])\n",
    "            print(\"Centers are {} and scales are {}\".format(center, scale))\n",
    "            centers.append(center)\n",
    "            scales.append(scale)\n",
    "\n",
    "        if len(pred_boxes) == 0:\n",
    "            frame_num += 1\n",
    "            if args.produce_vid:\n",
    "                video_tracked.write(image_debug)\n",
    "            continue\n",
    "\n",
    "        now = time.time()\n",
    "        print(\"SENDING IN TO POSES\", centers, scales)\n",
    "        pose_preds = get_pose_estimation_prediction(pose_model, image_pose, centers, scales, transform=pose_transform)\n",
    "        then = time.time()\n",
    "        \n",
    "        preds = []\n",
    "\n",
    "        for coords in pose_preds:\n",
    "\n",
    "            preds.append({\"keypoints\":[]})\n",
    "            for i, coord in enumerate(coords):\n",
    "                x_coord, y_coord = float(max(0, coord[0])), float(max(0, coord[1]))\n",
    "                preds[-1][\"keypoints\"].append((x_coord if x_coord > 0 else 0, y_coord if y_coord > 0 else 0))\n",
    "\n",
    "                if not (x_coord == 0 and y_coord == 0):\n",
    "                    x_coord, y_coord = int(x_coord), int(y_coord)\n",
    "                    if args.produce_vid:\n",
    "                        cv2.circle(image_debug, (x_coord, y_coord), 4, keypoint_colors[str(i + 1)], -1)\n",
    "                        cv2.putText(image_debug, str(i + 1), (x_coord - 4, y_coord - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        for it, box in enumerate(pred_boxes):\n",
    "            preds[it][\"bbox\"] = [float(box[0]), float(box[1]), float(box[2]), float(box[3])]\n",
    "            if args.tracking:\n",
    "                preds[it][\"bbox\"].append(float(predictions.scores[it].detach().cpu().numpy()))\n",
    "            if args.produce_vid:\n",
    "                cv2.rectangle(image_debug, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color=(0, 255, 0),\n",
    "                              thickness=3) \n",
    "\n",
    "        if args.produce_vid:\n",
    "            video_tracked.write(image_debug)\n",
    "        data[frame_num] = preds\n",
    "        frame_num += 1\n",
    "        \n",
    "    video.release()\n",
    "    if args.produce_vid:\n",
    "        video_tracked.release()\n",
    "        \n",
    "\n",
    "    json.dump(data, open(args.out_json, \"w\"))\n",
    "\n",
    "!pwd\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multitaskenv",
   "language": "python",
   "name": "multitaskenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
